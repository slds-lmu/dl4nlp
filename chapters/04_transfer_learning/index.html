<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/dl4nlp/css/style.css">


<title>Deep Learning for Natural Language Processing (DL4NLP) | Chapter 4: Transfer Learning</title>


<link rel="apple-touch-icon" sizes="180x180" href="/dl4nlp/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/dl4nlp/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/dl4nlp/favicon-16x16.png">
<link rel="manifest" href="/dl4nlp/site.webmanifest">
<link rel="mask-icon" href="/dl4nlp/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/dl4nlp/dl4nlp.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/dl4nlp">
        
        Home
        </a>
        
        <a class="nav-link" href="/dl4nlp/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/dl4nlp/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/dl4nlp/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/dl4nlp/references/">
        
        References
        </a>
        
        <a class="nav-link" href="/dl4nlp/team/">
        
        Team
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapter 4: Transfer Learning</h1>

<p><p>The term Transfer Learning generally refers to a special paradigm in machine learning, where the knowledge gained by an architecture while being trained on data from one task, domain or language is used to benefit training of the very same model on another task, domain or language. In the context of NLP, this mostly encapsulates <em>pre-training</em> on (unlabeled) general domain data in a self-supervised fashion and <em>fine-tuning</em> on (labaled) data from a specific task, domain or language.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://ruder.io/thesis/">Ruder (2019)</a></li>
</ul></p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_01_defs_challenges/">Chapter 4.1: Definitions and Challenges</a>
    
      
        <p>This chapter will explain the different paradigms which are commonly subsumed under the term Transfer Learning. It is important to be able to distinguish them from one another, since each one of them comes with its own challenges as well as its own use cases, where it is specifically useful.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_02_selfsup/">Chapter 4.2: Self-supervised pre-training</a>
    
      
        <p>Here we will introduce the concept of Self-Supervision, which is crucial for the current practice of Transfer Learning in NLP. It refers to a learning paradigm located somewhere between supervised and unsupervised learning, since it exhibits characterstics of both of them.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_03_datadriven/">Chapter 4.3: Data-driven tokenization</a>
    
      
        <p>Two of the three currently most frequently used data-driven tokenization algorithms will be introduced in this chapter. The remaining one, BPE, will be covered at the beginning of the next chapter.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_04_00_early_transfer/">Chapter 4.4: Early Transfer Learning architectures</a>
    
      
        <p>Before the use of the Transformer architecture (cf. Chapter 5) for Transfer Learning, two milestone architectures were published.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_04_01_elmo/">Chapter 4.4.1: ELMo</a>
    
      
        <p>In their paper Deep contextualized word representations, Peters et al. introduced a new class of word embeddings called ELMo (Embeddings from Language Models). Their unique feature is the ability to contextualize the embeddings in a bidirectional fashion (using biLSTMs), i.e. the representation of a word depends on the context on both sides. In order to obtain embeddings, whole sentences have to be fed to this architecture and the retrieved embeddings can then be used for a specific downstream task.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_04_02_ulmfit/">Chapter 4.4.2: ULMFiT</a>
    
      
        <p>In contrast to ELMo, this architecture is not able to produce bidirectionally contextual embeddings but only unidirectionally contextual ones (since unidirectional LSTM are employed). Apart from this, is relies on a different transfer learning paradigm. Instead of &amp;ldquo;only&amp;rdquo; generating embedding which can be used for downstream tasks, the whole architecture is fine-tuned towards a specific task. Hence the name Universal Language Model Fine-Tuning (ULMFiT).
</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://slds-lmu.github.io/i2ml/" target="_blank">I2ML Course</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_dl4nlp" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/dl4nlp" target="_blank">Website source code</a></li>
  
</ul>
</footer>



</body>
</html>
