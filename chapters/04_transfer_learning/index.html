<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/dl4nlp/css/style.css">


<title>Deep Learning for Natural Language Processing (DL4NLP) | Chapter 4: Transfer Learning</title>


<link rel="apple-touch-icon" sizes="180x180" href="/dl4nlp/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/dl4nlp/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/dl4nlp/favicon-16x16.png">
<link rel="manifest" href="/dl4nlp/site.webmanifest">
<link rel="mask-icon" href="/dl4nlp/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/dl4nlp/i2ml.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/dl4nlp">
        
        Home
        </a>
        
        <a class="nav-link" href="/dl4nlp/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/dl4nlp/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/dl4nlp/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/dl4nlp/references/">
        
        References
        </a>
        
        <a class="nav-link" href="/dl4nlp/team/">
        
        Team
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapter 4: Transfer Learning</h1>

<p><p>blabla</p>
</p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_01_00_challenges/">Chapter 4.1: Challenges</a>
    
      
        <p>blabla
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_01_01_low_resource/">Chapter 4.1.1: Low-resource environments</a>
    
      
        <p>blabla
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_01_02_crosslingual/">Chapter 4.2.2: Cross-lingual transfer</a>
    
      
        <p>blabla
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_01_03_tasks_datasets/">Chapter 4.1.3: Challenging tasks and data sets</a>
    
      
        <p>blabla
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_01_04_selfsup/">Chapter 4.1.4: Self-supervised pre-training</a>
    
      
        <p>blabla
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_02_00_datadriven/">Chapter 4.2: Data-driven tokenization</a>
    
      
        <p>blabla
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_02_01_wordpiece/">Chapter 4.2.1: WordPiece tokenization</a>
    
      
        <p>blabla
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_02_02_sentencepiece/">Chapter 4.2.2: SentencePiece</a>
    
      
        <p>blabla
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_03_00_early_transfer/">Chapter 4.3: Early Transfer Learning architectures</a>
    
      
        <p>Before the use of the Transformer architecture (cf. Chapter 5) for Transfer Learning, two milestone architectures were published.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_03_01_elmo/">Chapter 4.3.1: ELMo</a>
    
      
        <p>In their paper Deep contextualized word representations, Peters et al. introduced a new class of word embeddings called ELMo (Embeddings from Language Models). Their unique feature is the ability to contextualize the embeddings in a bidirectional fashion (using biLSTMs), i.e. the representation of a word depends on the context on both sides. In order to obtain embeddings, whole sentences have to be fed to this architecture and the retrieved embeddings can then be used for a specific downstream task.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/04_03_02_ulmfit/">Chapter 4.3.2: ULMFiT</a>
    
      
        <p>In contrast to ELMo, this architecture is not able to produce bidirectionally contextual embeddings but only unidirectionally contextual ones (since unidirectional LSTM are employed). Apart from this, is relies on a different transfer learning paradigm. Instead of &amp;ldquo;only&amp;rdquo; generating embedding which can be used for downstream tasks, the whole architecture is fine-tuned towards a specific task. Hence the name Universal Language Model Fine-Tuning (ULMFiT).
</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="/dl4nlp" target="_blank">Main Course Website</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_dl4nlp" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/dl4nlp" target="_blank">Website source code</a></li>
  
</ul>
</footer>



</body>
</html>
