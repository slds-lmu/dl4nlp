<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 4: Transfer Learning on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/</link><description>Recent content in Chapter 4: Transfer Learning on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 4.1: Definitions and Challenges</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_defs_challenges/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_defs_challenges/</guid><description>&lt;p>This chapter will explain the different paradigms which are commonly subsumed under the term Transfer Learning. It is important to be able to distinguish them from one another, since each one of them comes with its own challenges as well as its own use cases, where it is specifically useful.&lt;/p></description></item><item><title>Chapter 4.2: Self-supervised pre-training</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_02_selfsup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_02_selfsup/</guid><description>&lt;p>Here we will introduce the concept of Self-Supervision, which is crucial for the current practice of Transfer Learning in NLP.
It refers to a learning paradigm located somewhere between supervised and unsupervised learning, since it exhibits characterstics of both of them.&lt;/p></description></item><item><title>Chapter 4.3: Data-driven tokenization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_03_datadriven/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_03_datadriven/</guid><description>&lt;p>Two of the three currently most frequently used data-driven tokenization algorithms will be introduced in this chapter. The remaining one, BPE, will be covered at the beginning of the next chapter.&lt;/p></description></item><item><title>Chapter 4.4: Early Transfer Learning architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_04_00_early_transfer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_04_00_early_transfer/</guid><description>&lt;p>Before the use of the Transformer architecture (&lt;a href="https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/">cf. Chapter 5&lt;/a>) for Transfer Learning, two milestone architectures were published.&lt;/p></description></item><item><title>Chapter 4.4.1: ELMo</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_04_01_elmo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_04_01_elmo/</guid><description>&lt;p>In their paper &lt;em>Deep contextualized word representations&lt;/em>, Peters et al. introduced a new class of word embeddings called ELMo (&lt;em>E&lt;/em>mbeddings from &lt;em>L&lt;/em>anguage &lt;em>Mo&lt;/em>dels).
Their unique feature is the ability to contextualize the embeddings in a bidirectional fashion (using biLSTMs), i.e. the representation of a word depends on the context on both sides. In order to obtain embeddings, whole sentences have to be fed to this architecture and the retrieved embeddings can then be used for a specific downstream task.&lt;/p></description></item><item><title>Chapter 4.4.2: ULMFiT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_04_02_ulmfit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_04_02_ulmfit/</guid><description>&lt;p>In contrast to ELMo, this architecture is not able to produce bidirectionally contextual embeddings but only unidirectionally contextual ones (since unidirectional LSTM are employed). Apart from this, is relies on a different transfer learning paradigm. Instead of &amp;ldquo;only&amp;rdquo; generating embedding which can be used for downstream tasks, the whole architecture is &lt;em>fine-tuned&lt;/em> towards a specific task. Hence the name &lt;em>U&lt;/em>niversal &lt;em>L&lt;/em>anguage &lt;em>M&lt;/em>odel &lt;em>Fi&lt;/em>ne-&lt;em>T&lt;/em>uning (ULMFiT).&lt;/p></description></item></channel></rss>