<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 5: Post-BERT Era on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/</link><description>Recent content in Chapter 5: Post-BERT Era on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo 0.125.0</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 5.1: Implications for future work &amp; BERTology</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_01_bertology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_01_bertology/</guid><description>&lt;p>BERTs architecture was very impactful and changed research in this field. This chapter also gives a glimpse into BERTology.&lt;/p></description></item><item><title>Chapter 5.2: BERT-based architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_02_bert_based/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_02_bert_based/</guid><description>&lt;p>By changing the pretraining objectives you can create new architectures. Here we introduce RoBERTa and ALBERT.&lt;/p></description></item><item><title>Chapter 5.3: Model distillation</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_03_distilbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_03_distilbert/</guid><description>&lt;p>This chapter introduces the concept of model distillation as a way to create more efficient models.&lt;/p></description></item></channel></rss>