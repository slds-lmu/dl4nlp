<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 6: Post-BERT Era 2 and using the Transformer on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/</link><description>Recent content in Chapter 6: Post-BERT Era 2 and using the Transformer on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 6.1: Post-BERT architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_01_postbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_01_postbert/</guid><description>&lt;p>This chapter will introduce two new architectures from the post-BERT era, namely ELECTRA and XLNet. By changing the pre training approach we can create new models.&lt;/p></description></item><item><title>Chapter 6.2: Tasks as text-to-text problem</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_02_text2text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_02_text2text/</guid><description>&lt;p>BERT has many shortcomings which is why we reformulate every classification task as a text-to-text problem.&lt;/p></description></item><item><title>Chapter 6.3: BERT-based architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_03_t5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_03_t5/</guid><description>&lt;p>Here we introduce T5 (&lt;strong>T&lt;/strong>ext-&lt;strong>T&lt;/strong>o-&lt;strong>T&lt;/strong>ext &lt;strong>T&lt;/strong>ransfer &lt;strong>T&lt;/strong>ransformer) a complete encoder-decoder Transformer architecture, which reformulates every task as text-to-text tasks.&lt;/p></description></item></channel></rss>