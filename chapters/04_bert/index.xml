<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 4: BERT on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/</link><description>Recent content in Chapter 4: BERT on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/04_bert/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 4.1: ARLMs vs. MLM</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_01_arlm_mlm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_01_arlm_mlm/</guid><description>&lt;p>This chapter will introduce the concept of self supervision and explain the difference between &lt;strong>A&lt;/strong>utoregressive &lt;strong>L&lt;/strong>anguage &lt;strong>M&lt;/strong>odelling and &lt;strong>M&lt;/strong>asked &lt;strong>L&lt;/strong>anguage &lt;strong>M&lt;/strong>odelling.&lt;/p></description></item><item><title>Chapter 4.2: Measuring Performance</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_02_metrics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_02_metrics/</guid><description>&lt;p>Here we discuss various ways of measuring the performance of language models.&lt;/p></description></item><item><title>Chapter 4.3: The Architecture</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_03_corefacts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_03_corefacts/</guid><description>&lt;p>In this chapter we explain the architecture of BERT and how it uses the Encoder of the Transformer.&lt;/p></description></item><item><title>Chapter 4.4: Pre-training &amp; Fine-Tuning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_04_pretrain_finetune/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_04_pretrain_finetune/</guid><description>&lt;p>This chapter discusses the methods used for pre-training of BERT, such as masked language modelling and next sentence prediction. It also briefly mentions how BERT has been finetuned for different tasks.&lt;/p></description></item><item><title>Chapter 4.5: Transfer Learning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_05_transferlearning_selfsup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_05_transferlearning_selfsup/</guid><description>&lt;p>This chapter briefly introduces the concept of Transfer Learning .&lt;/p></description></item></channel></rss>