<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/dl4nlp/css/style.css">


<title>Deep Learning for Natural Language Processing (DL4NLP) | Chapter 7: Generative Pre-Trained Transformers</title>


<link rel="apple-touch-icon" sizes="180x180" href="/dl4nlp/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/dl4nlp/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/dl4nlp/favicon-16x16.png">
<link rel="manifest" href="/dl4nlp/site.webmanifest">
<link rel="mask-icon" href="/dl4nlp/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/dl4nlp/dl4nlp.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/dl4nlp">
        
        Home
        </a>
        
        <a class="nav-link" href="/dl4nlp/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/dl4nlp/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/dl4nlp/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/dl4nlp/references/">
        
        References
        </a>
        
        <a class="nav-link" href="/dl4nlp/team/">
        
        Team
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapter 7: Generative Pre-Trained Transformers</h1>

<p><p>In this chapter, you will learn about the evolution of the GPT series, spanning from GPT-1 to GPT-3, which revolutionizes natural language processing by employing generative transformer architectures pre-trained on massive text corpora to generate contextually relevant text.</p>
<h3 id="additional-resources">Additional Resources</h3>
<ul>
<li><a href="https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2">Overview of GPT-1 until GPT-3</a></li>
</ul>
</p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/dl4nlp/chapters/07_gpt/07_01_gpt/">Chapter 07.01: GPT-1 (2018)</a>
    
      
        <p>GPT-1 [1] introduces a novel approach to natural language processing by employing a generative transformer architecture pre-trained on a vast corpus of text data, where task-specific input transformations are performed to adapt the model to different tasks. By fine-tuning the model on task-specific data with minimal changes to the architecture, GPT-1 demonstrates the effectiveness of transfer learning and showcases the potential of generative transformers in a wide range of natural language understanding and generation tasks.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/07_gpt/07_02_gpt2/">Chapter 07.02: GPT-2 (2019)</a>
    
      
        <p>GPT-2 [1] builds upon its predecessor with a larger model size, more training data, and improved architecture. Like GPT-1, GPT-2 utilizes a generative transformer architecture but features a significantly increased number of parameters, leading to enhanced performance in language understanding and generation tasks. Additionally, GPT-2 introduces a scaled-up version of the training data and fine-tuning techniques to further refine its language capabilities.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/07_gpt/07_03_gpt3xshot/">Chapter 07.03: GPT-3 (2020) &amp; X-shot learning</a>
    
      
        <p>In this chapter, we&rsquo;ll explore GPT-3 [1]. GPT-3 builds on the successes of its predecessors, boasting a massive architecture and extensive pre-training on diverse text data. Unlike previous models, GPT-3 introduces a few-shot learning approach, allowing it to perform tasks with minimal task-specific training data. With its remarkable scale and versatility, GPT-3 represents a significant advancement in natural language processing, showcasing the potential of large-scale transformer architectures in various applications.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/07_gpt/07_04_tasks/">Chapter 07.04: Tasks &amp; Performance</a>
    
      
        <p>GPT-3 has X-shot abilities, meaning it is able to perform tasks with minimal or even no task-specific training data. This chapter provides an overview over various different tasks and illustrates the X-shot capabilities of GPT-3. Additionally you will be introduced to relevant benchmarks.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/07_gpt/07_05_discussion/">Chapter 07.05: Discussion: Ethics and Cost</a>
    
      
        <p>In discussing GPT-3&rsquo;s ethical implications, it is crucial to consider its potential societal impact, including issues surrounding bias, misinformation, and data privacy. With its vast language generation capabilities, GPT-3 has the potential to disseminate misinformation at scale, posing risks to public trust and safety. Additionally, the model&rsquo;s reliance on large-scale pretraining data raises concerns about reinforcing existing biases present in the data, perpetuating societal inequalities. Furthermore, the use of GPT-3 in sensitive applications such as content generation, automated customer service, and decision-making systems raises questions about accountability, transparency, and unintended consequences. As such, responsible deployment of GPT-3 requires careful consideration of ethical guidelines, regulatory frameworks, and robust mitigation strategies to address these challenges and ensure the model&rsquo;s ethical use in society.
</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://slds-lmu.github.io/i2ml/" target="_blank">I2ML Course</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_dl4nlp" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/dl4nlp" target="_blank">Website source code</a></li>
  
</ul>
</footer>



</body>
</html>
