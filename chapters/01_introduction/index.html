<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/dl4nlp/css/style.css">


<title>Deep Learning for Natural Language Processing (DL4NLP) | Chapter 1: Introduction to the course</title>


<link rel="apple-touch-icon" sizes="180x180" href="/dl4nlp/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/dl4nlp/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/dl4nlp/favicon-16x16.png">
<link rel="manifest" href="/dl4nlp/site.webmanifest">
<link rel="mask-icon" href="/dl4nlp/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/dl4nlp/dl4nlp.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/dl4nlp">
        
        Home
        </a>
        
        <a class="nav-link" href="/dl4nlp/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/dl4nlp/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/dl4nlp/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/dl4nlp/references/">
        
        References
        </a>
        
        <a class="nav-link" href="/dl4nlp/team/">
        
        Team
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapter 1: Introduction to the course</h1>

<p><p>In this chapter, you&rsquo;ll dive into the fundamental principles of Deep Learning for Natural Language Processing (NLP). Explore key concepts including learning paradigms, various tasks within NLP, the neural probabilistic language model, and the significance of embeddings.</p></p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/dl4nlp/chapters/01_introduction/01_01_course_intro/">Chapter 01.01: Introduction and Course Outline</a>
    
      
        <p>This chapter introduces the people responsible for the course, aims to answer all open question and should give an impression of the expected workload.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/01_introduction/01_02_learningparadigms/">Chapter 01.02: Learning Paradigms</a>
    
      
        <p>This chapter introduces different learning paradigms, such as embeddings, prompting and pre-training and finetuning a model. These are all very important concepts in the context of Deep Learning for NLP. The purpose of this chapter is to give an overview over these concepts.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/01_introduction/01_03_tasks/">Chapter 01.03: NLP tasks</a>
    
      
        <p>Here you will learn about the distinctions between low-level and high-level tasks, as well as the differences between linguistic tasks and broader classification tasks.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/01_introduction/01_04_nplm/">Chapter 01.04: Neural Probabalistic Language Model</a>
    
      
        <p>In this chapter, take your first steps into the world of Deep Neural Networks for language modeling with the Neural Probabilistic Language Model (NPLM) [1]. Explore how NPLM represents a foundational approach in utilizing deep learning techniques for understanding and generating natural language. Uncover the principles behind NPLM and its significance in paving the way for more sophisticated language models.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/01_introduction/01_05_embeddings/">Chapter 01.05: Word Embeddings</a>
    
      
        <p>Embeddings allow us to represent tokens with a vector representation, enabling computers to efficiently understand the meaning and context of different words. In this chapter we will first explain the general concept of embeddings and then introduce two popular approaches, namely Word2Vec [1] and FastText [2]. Word2vec is a neural network model that learns distributed representations of words in a continuous vector space based on their contextual usage in a corpus of text. Word2vec achieves this by training on a large dataset to predict the surrounding words given a target word, embedding each word as a dense vector where similar words are closer together in the vector space. FastText is an extension of Word2vec that represents words as bags of character n-grams, enabling it to capture morphological information alongside semantics. It accomplishes this by breaking down each word into character n-grams, creating embeddings for these subword units, and then summing or averaging these embeddings to obtain the representation for the whole word.
</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://slds-lmu.github.io/i2ml/" target="_blank">I2ML Course</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_dl4nlp" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/dl4nlp" target="_blank">Website source code</a></li>
  
</ul>
</footer>



</body>
</html>
