<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 6: Using the Transformer on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/</link><description>Recent content in Chapter 6: Using the Transformer on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 6.1: Transformer Decoder - GPT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_01_gpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_01_gpt/</guid><description>&lt;p>This chapter will explain how the decoder part of the transformer was first used as the backbone for pre-training a language model. Radford et al. (2018) pre-trained a multi-layer transformer decoder using the language modeling objective on a large unannotated corpus (BooksCorpus; Zhu et al., 2015).&lt;/p></description></item><item><title>Chapter 6.2: Transformer Encoder - BERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_00_bert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_00_bert/</guid><description>&lt;p>This chapter will explain how the encoder part of the transformer was first used as the backbone for pre-training a language model. Devlin et al. (2019) came
up with a new objective, since the unconstrained self-attention in the encoder makes it impossible to train an encoder-based model on the language modeling objective
in a meaningful way. The resulting model (BERT) was first published as a pre-print in 10/2018 and presented at the ACL conference in 06/2019.&lt;/p></description></item><item><title>Chapter 6.2.1: BERT - Architectural details</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_01_architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_01_architecture/</guid><description>&lt;p>This chapter will explain the architectural details of BERT - from the (context-free) embedding layer until the final classification layer.&lt;/p></description></item><item><title>Chapter 6.2.2: BERT - Pre-training regime</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_02_pretrain/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_02_pretrain/</guid><description>&lt;p>This chapter will explain how BERT was pre-trained.&lt;/p></description></item><item><title>Chapter 6.2.3: BERT - Fine-tuning procedure</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_03_finetune/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_03_finetune/</guid><description>&lt;p>This chapter will explain how BERT can be fine-tuned on a specific task using labeled training data.&lt;/p></description></item><item><title>Chapter 6.2.4: BERT - Shortcomings &amp; Critique</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_04_short/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_04_short/</guid><description>&lt;p>This chapter will explain the shortcomings of BERT and explain the critique which this approach faces.&lt;/p></description></item><item><title>Chapter 6.2.5: BERT - Implications for future work</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_05_implications/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_05_implications/</guid><description>&lt;p>This chapter will address the learnings and the implications for future research which researchers took away from analyzing BERT.&lt;/p></description></item><item><title>Chapter 6.3: BERT-based architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_00_bert_based/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_00_bert_based/</guid><description>&lt;p>Following the advent of BERT, a great focus in this research are was on extending, explaining or improving it.
This chapter will explain three of the most important architectures from this streamline of reseach called BERTology&lt;/p></description></item><item><title>Chapter 6.3.1: RoBERTa</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_01_roberta/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_01_roberta/</guid><description>&lt;p>This chapter is about RoBERTa, an optimized version of the initial BERT architecture.&lt;/p></description></item><item><title>Chapter 6.3.2: ALBERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_02_albert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_02_albert/</guid><description>&lt;p>This chapter is about ALBERT, a more efficient version of the initial BERT architecture du to the application of parameter sharing techniques.&lt;/p></description></item><item><title>Chapter 6.4: Post-BERT architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_00_postbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_00_postbert/</guid><description>&lt;p>blabla&lt;/p></description></item><item><title>Chapter 6.4.1: Encoder architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_01_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_01_encoder/</guid><description>&lt;p>This chapter will cover a set of transformer-encoder based archtiectures like ELECTRA, ERNIE or BigBird.&lt;/p></description></item></channel></rss>