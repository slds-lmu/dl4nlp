<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 6: Using the Transformer on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/</link><description>Recent content in Chapter 6: Using the Transformer on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 6.1: Transformer Decoder - GPT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_01_gpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_01_gpt/</guid><description>&lt;p>This chapter will explain how the decoder part of the transformer was first used as the backbone for pre-training a language model. Radford et al. (2018) pre-trained a multi-layer transformer decoder using the language modeling objective on a large unannotated corpus (BooksCorpus; Zhu et al., 2015).&lt;/p></description></item><item><title>Chapter 6.2: Transformer Encoder - BERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_00_bert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_00_bert/</guid><description>&lt;p>This chapter will explain how the encoder part of the transformer was first used as the backbone for pre-training a language model. Devlin et al. (2019) came
up with a new objective, since the unconstrained self-attention in the encoder makes it impossible to train an encoder-based model on the language modeling objective
in a meaningful way. The resulting model (BERT) was first published as a pre-print in 10/2018 and presented at the ACL conference in 06/2019.&lt;/p></description></item><item><title>Chapter 6.2.1: BERT - Architectural details</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_01_architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_01_architecture/</guid><description>&lt;p>This chapter will explain the architectural details of BERT - from the (context-free) embedding layer until the final classification layer.&lt;/p></description></item><item><title>Chapter 6.2.2: BERT - Pre-training regime</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_02_pretrain/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_02_pretrain/</guid><description>&lt;p>This chapter will explain how BERT was pre-trained.&lt;/p></description></item><item><title>Chapter 6.2.3: BERT - Fine-tuning procedure</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_03_finetune/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_03_finetune/</guid><description>&lt;p>This chapter will explain how BERT can be fine-tuned on a specific task using labeled training data.&lt;/p></description></item><item><title>Chapter 6.2.4: BERT - Shortcomings &amp; Critique</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_04_short/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_04_short/</guid><description>&lt;p>This chapter will explain the shortcomings of BERT and explain the critique which this approach faces.&lt;/p></description></item><item><title>Chapter 6.2.5: BERT - Implications for future work</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_05_implications/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_05_implications/</guid><description>&lt;p>This chapter will address the learnings and the implications for future research which researchers took away from analyzing BERT.&lt;/p></description></item><item><title>Chapter 6.3: BERT-based architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_00_bert_based/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_00_bert_based/</guid><description>&lt;p>Following the advent of BERT, a great focus in this research are was on extending, explaining or improving it.
This chapter will explain three of the most important architectures from this streamline of reseach called BERTology&lt;/p></description></item><item><title>Chapter 6.3.1: RoBERTa</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_01_roberta/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_01_roberta/</guid><description>&lt;p>This chapter is about RoBERTa, an optimized version of the initial BERT architecture.&lt;/p></description></item><item><title>Chapter 6.3.2: ALBERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_02_albert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_02_albert/</guid><description>&lt;p>This chapter is about ALBERT, a more efficient version of the initial BERT architecture du to the application of parameter sharing techniques.&lt;/p></description></item><item><title>Chapter 6.3.3: Model distillation - DistilBERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_03_distilbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_03_distilbert/</guid><description>&lt;p>This chapter is about DistilBERT, an architecture derived from the original BERT model via model distillation.
We will explain the process of model distillation itself as well as the created architecture.&lt;/p></description></item><item><title>Chapter 6.3.4: Add-On: Model Recycling</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_04_recycling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_04_recycling/</guid><description>&lt;p>This chapter is about about a very recent project on the question which model to choose for fine-tuning.&lt;/p></description></item><item><title>Chapter 6.4: Post-BERT architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_00_postbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_00_postbert/</guid><description>&lt;p>This chapter covers a selection of model published after BERT, but which also still rely on the pre-train/fine-tune paradigm.&lt;/p></description></item><item><title>Chapter 6.4.1: Encoder architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_01_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_01_encoder/</guid><description>&lt;p>This chapter will cover a set of transformer-encoder based archtiectures like ELECTRA or BigBird. At the moment, there are only slides on ELECTRA, whereas BigBird is still part of the efficient transformers slide deck and will be added here in the future.&lt;/p></description></item><item><title>Chapter 6.4.2: Decoder architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_02_decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_02_decoder/</guid><description>&lt;p>This chapter will cover a set of transformer-decoder based archtiectures, first and foremost GPT&amp;rsquo;s two successors and the XLNet architecture and its premutation language modeling objective.&lt;/p></description></item><item><title>Chapter 6.4.3: Encoder-Decoder architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_03_encdec/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_03_encdec/</guid><description>&lt;p>This chapter will cover a set of models using the whole transformer architecture again.&lt;/p></description></item><item><title>Chapter 6.5: Efficient Transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_05_efficient/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_05_efficient/</guid><description>&lt;p>This chapter will cover a set of models trying to decrease the computational complexity of the self-attention mechanism in the transformer architecture,
since it represents the backbone of nearly all state-of-the-art models and thus also limits these models to some extent.&lt;/p></description></item></channel></rss>