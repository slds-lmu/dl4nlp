<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 6: Using the Transformer on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/</link><description>Recent content in Chapter 6: Using the Transformer on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 6.1: Transformer Decoder - GPT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_01_gpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_01_gpt/</guid><description>&lt;p>This chapter will explain how the decoder part of the transformer was first used as the backbone for pre-training a language model. Radford et al. (2018) pre-trained a multi-layer transformer decoder using the language modeling objective on a large unannotated corpus (BooksCorpus; Zhu et al., 2015).&lt;/p></description></item></channel></rss>