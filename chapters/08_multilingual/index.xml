<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 8: Multilinguality on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/</link><description>Recent content in Chapter 8: Multilinguality on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 8.1: Why Multilinguality?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_01_why_multilinguality/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_01_why_multilinguality/</guid><description>&lt;p>So far, we have seen interesting use cases for NLP. Clearly, these can be extended to multiple languages. But there are also interesting problems across languages. In this part we present a general motivation and a classification of problem types.&lt;/p></description></item><item><title>Chapter 8.2: Cross-lingual Word Embeddings</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_02_cross_lingual_word_embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_02_cross_lingual_word_embeddings/</guid><description>&lt;p>In this session, multilingual word embeddings are discussed. We describe the two main training strategies and look at examples of models to train those. Additionally, we look at unsupervised learning of multi-lingual word embeddings.&lt;/p></description></item><item><title>Chapter 8.3: (Massively) Multilingual transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_03_multi_lingual_transformers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_03_multi_lingual_transformers/</guid><description>&lt;p>As we have previously seen, transformers are the working horse for modern NLP. In this section we learn how transformers are adapted to work in multilingual settings and discuss typical issues when training those. Furthermore we look at zero-shot cross lingual transfer capabilities.&lt;/p></description></item></channel></rss>