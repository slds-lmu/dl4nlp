<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 8: Decoding Strategies on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_decoding/</link><description>Recent content in Chapter 8: Decoding Strategies on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/08_decoding/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 08.01: What is Decoding?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_decoding/08_01_intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_decoding/08_01_intro/</guid><description>&lt;p>Here we introduce the concept of decoding. Given a prompt and a generative language model, how does it generate text? The model produces a probability distribution over all tokens in the vocabulary. The way the model uses that probability distribution to generate the next token is what is called a decoding strategy.&lt;/p></description></item><item><title>Chapter 08.02: Greedy &amp; Beam Search</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_decoding/08_02_determ/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_decoding/08_02_determ/</guid><description>&lt;p>Here we introduce two deterministic decoding strategies, greedy &amp;amp; beam search. Both methods are determenistic, which means there is no sampling involved when generating text. While greedy decoding always chooses the token with the highest probability, beam search keeps track of multiple beams to generate the next token.&lt;/p></description></item><item><title>Chapter 08.03: Stochastic Decoding &amp; CS/CD</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_decoding/08_03_sampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_decoding/08_03_sampling/</guid><description>&lt;p>In this chapter you will learn about more methods beyond simple deterministic decoding strategies. We introduce sampling with temperature, where you add a temperature parameter into the softmax formula, top-k [1] and top-p [2] sampling, where you sample from a set of top tokens and finally contrastive search [3] and contrastive decoding [4].&lt;/p></description></item><item><title>Chapter 08.04: Decoding Hyperparameters &amp; Practical considerations</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_decoding/08_04_hyper_param/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_decoding/08_04_hyper_param/</guid><description>&lt;p>In this chapter you will learn how to use the different decoding strategies in practice. When using models from huggingface you can choose the decoding strategy by specifying the hyperparameters of the &lt;code>generate&lt;/code> method of those models.&lt;/p></description></item><item><title>Chapter 08.05: Evaluation Metrics</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_decoding/08_05_eval_metrics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_decoding/08_05_eval_metrics/</guid><description>&lt;p>Here we answer the question on how to evaluate the generated outputs in open ended text generation. We first explain &lt;strong>BLEU&lt;/strong> [1] and &lt;strong>ROUGE&lt;/strong> [2], which are metrics for tasks with a gold reference. Then we introduce &lt;strong>diversity&lt;/strong>, &lt;strong>coherence&lt;/strong> [3] and &lt;strong>MAUVE&lt;/strong> [4], which are metrics for tasks without a gold reference such as open ended text generation. You will also learn about human evaluation.&lt;/p></description></item></channel></rss>