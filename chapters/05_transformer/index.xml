<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 5: Transformer on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/</link><description>Recent content in Chapter 5: Transformer on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 5.1: Byte-Pair Encoding (BPE)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_01_bpe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_01_bpe/</guid><description>Understanding BPE is crucial for understanding the transformer architecture, since it is used to &amp;rsquo;learn&amp;rsquo; the vocabulary for the embedding layer.
Instead of using a simple heuristic (characters or words), the tokens which constitute the vocabulary are formed base on the training corpus.
Concept 3 slides, + 1 slides w/ example of 2-3 steps of merging, 8 minutes
References Gage (1994) Sennrich et al. (2016)</description></item></channel></rss>