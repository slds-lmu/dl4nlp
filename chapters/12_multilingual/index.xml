<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 12: Multilinguality on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/12_multilingual/</link><description>Recent content in Chapter 12: Multilinguality on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/12_multilingual/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 12.01: Why Multilinguality?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/12_multilingual/12_01_why_multilinguality/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/12_multilingual/12_01_why_multilinguality/</guid><description>&lt;p>We need multilingual models to bridge language barriers, enhance global communication, and ensure equitable access to information and technology across diverse linguistic communities. These models enable seamless translation, cross-lingual information retrieval, and multi-language support in applications, allowing people to interact with technology in their native languages. By addressing the challenges of linguistic diversity, multilingual models promote inclusivity, facilitate international collaboration, and democratize access to digital resources and services globally.&lt;/p></description></item><item><title>Chapter 12.02: Cross-lingual Word Embeddings</title><link>https://slds-lmu.github.io/dl4nlp/chapters/12_multilingual/12_02_cross_lingual_word_embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/12_multilingual/12_02_cross_lingual_word_embeddings/</guid><description>&lt;p>Cross-lingual word embeddings create a shared vector space for words from multiple languages, allowing models to understand and process text across different languages seamlessly. In this chapter we describe the two main training strategies and look at examples of models to train those. Additionally, we look at unsupervised learning of multi-lingual word embeddings.&lt;/p></description></item><item><title>Chapter 12.03: (Massively) Multilingual Transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/12_multilingual/12_03_multi_lingual_transformers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/12_multilingual/12_03_multi_lingual_transformers/</guid><description>&lt;p>As we have previously seen, transformers are the working horse for modern NLP. In this section we learn how transformers are adapted to work in multilingual settings and discuss typical issues when training those. Furthermore we look at zero-shot cross lingual transfer capabilities.&lt;/p></description></item></channel></rss>