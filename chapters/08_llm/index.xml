<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 8: Large Language Models (LLMs) on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/</link><description>Recent content in Chapter 8: Large Language Models (LLMs) on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/08_llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 8.1: Instruction Fine-Tuning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_01_instruction_tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_01_instruction_tuning/</guid><description>&lt;p>In this chapter we introduce instruction-tuning, which is a technique that allows us to adapt the models to follow instructions.&lt;/p></description></item><item><title>Chapter 8.2: Chain-of-thought Prompting</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_02_cot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_02_cot/</guid><description>&lt;p>In this session we cover Chain-of-thoght Prompting, which is a technique to improve the performance of models without requiring additional training.&lt;/p></description></item><item><title>Chapter 8.3: Emergent Abilities</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_03_emerging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_03_emerging/</guid><description>&lt;p>Various researchers have reported that LLMs seem to have emergent abilities. In this section we discuss the concept of emergence in LLMs.&lt;/p></description></item></channel></rss>