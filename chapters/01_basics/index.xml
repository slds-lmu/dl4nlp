<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 1: ML Basics on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/</link><description>Recent content in Chapter 1: ML Basics on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/01_basics/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 01.01: What is ML?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-01-what_is_ml/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-01-what_is_ml/</guid><description>&lt;p>As subtopic of artificial intelligence, machine learning is a mathematically well-defined discipline and usually constructs predictive or decision models from data, instead of explicitly programming them. In this section, you will see some typical examples of where machine learning is applied and the main directions of machine learning.&lt;/p></description></item><item><title>Chapter 01.02: Data</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-02-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-02-data/</guid><description>&lt;p>In this section we explain the basic structure of tabular data used in machine learning. We will differentiate targets from features, talk about labeled and unlabeled data and introduce the concept of the data generating process.&lt;/p></description></item><item><title>Chapter 01.03: Tasks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-03-tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-03-tasks/</guid><description>&lt;p>The tasks of supervised learning can roughly be divided in two categories: regression (for continuous outcome) and classification (for categorical outcome). We will present some examples.&lt;/p></description></item><item><title>Chapter 01.04: Models and Parameters</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-04-models-parameters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-04-models-parameters/</guid><description>&lt;p>We introduce models as functional hypotheses about the mapping from feature to target space that allow us to make predictions by computing a function of the input data. Frequently in machine learning, models are understood to be parameterized curves, which is illustrated by several examples.&lt;/p></description></item><item><title>Chapter 01.05: Learner</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-05-learner/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-05-learner/</guid><description>&lt;p>Roughly speaking, learners (endowed with a specific hyperparameter configuration) take training data and return a model.&lt;/p></description></item><item><title>Chapter 01.06: Losses and Risk Minimization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-06-riskminimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-06-riskminimization/</guid><description>&lt;p>In order to find good solutions we need a concept to evaluate and compare models. To this end, the concepts of &lt;em>loss function&lt;/em>, &lt;em>risk&lt;/em> and &lt;em>empirical risk minimization&lt;/em> are introduced.&lt;/p></description></item><item><title>Chapter 01.07: Optimization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-07-optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-07-optimization/</guid><description>&lt;p>In this section we study parameter optimization as computational solution to machine learning problems. We address pitfalls in non-convex optimization problems and introduce the fundamental concept of gradient descent.&lt;/p></description></item><item><title>Chapter 01.08: Components of a Learner</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-08-learnercomponents-hro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-08-learnercomponents-hro/</guid><description>&lt;p>Nearly all supervised learning algorithms can be described in terms of three components: 1) hypothesis space, 2) risk, and 3) optimization. In this section, we explain how these components interact and why this is a very useful concept for many supervised learning approaches.&lt;/p></description></item><item><title>Chapter 01.09: Loss Functions for Regression</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-09-losses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-09-losses/</guid><description>&lt;p>\(L1\) and \(L2\) are two essential loss functions used for evaluating the performance of regression models. This section defines \(L1\) and \(L2\) loss and explains the differences.&lt;/p></description></item><item><title>Chapter 01.10: Linear Regression Models</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-10-linearmodel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-10-linearmodel/</guid><description>&lt;p>In this section, we explain how the linear regression model can be used from a machine learning perspective to predict a continuous numerical target variable. We use the concepts of loss function and empirical risk minimization to find the linear model that best fits the data.&lt;/p></description></item><item><title>Chapter 01.11: Classification Tasks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-11-tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-11-tasks/</guid><description>&lt;p>In classification, the task is to predict a categorical (binary or multiclass) label. In this section, we illustrate the concept of classification with some typical examples.&lt;/p></description></item><item><title>Chapter 01.12: Basic Definitions</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-12-classification-basicdefs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-12-classification-basicdefs/</guid><description>&lt;p>Although we are primarily interested in actual class labels, classification models usually output scores or probabilities first. We will explain why, introduce the concepts of decision regions and decision boundaries, and discern two fundamental approaches to constructing classifiers: the generative approach and the discriminant approach.&lt;/p></description></item><item><title>Chapter 01.13: Linear Classifiers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-13-classification-linear/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-13-classification-linear/</guid><description>&lt;p>Linear classifiers are an essential subclass of classification models. This section provides the definition of a linear classifier and depicts differences between linear and non-linear decision boundaries.&lt;/p></description></item><item><title>Chapter 01.14: Logistic Regression</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/13-14-classification-logistic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/13-14-classification-logistic/</guid><description>&lt;p>Logistic regression is a discriminant approach toward constructing a classifier. We will motivate logistic regression via the logistic function, define the log-loss for optimization and illustrate the approach in 1D and 2D.&lt;/p></description></item></channel></rss>