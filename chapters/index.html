<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/dl4nlp/css/style.css">


<title>Deep Learning for Natural Language Processing (DL4NLP) | Chapters</title>


<link rel="apple-touch-icon" sizes="180x180" href="/dl4nlp/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/dl4nlp/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/dl4nlp/favicon-16x16.png">
<link rel="manifest" href="/dl4nlp/site.webmanifest">
<link rel="mask-icon" href="/dl4nlp/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/dl4nlp/dl4nlp.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/dl4nlp">
        
        Home
        </a>
        
        <a class="nav-link" href="/dl4nlp/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/dl4nlp/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/dl4nlp/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/dl4nlp/references/">
        
        References
        </a>
        
        <a class="nav-link" href="/dl4nlp/team/">
        
        Team
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapters</h1>

<p></p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/dl4nlp/chapters/00_basics/">Chapter 0: Machine Learning Basics</a>
    
      
        <p>This chapter introduces the basic concepts of Machine Learning. We therefore rely the excellent material from the I2ML Course which already comes with videos and has been taught LMU numerous times already.
The focus of these chapters in on introducing supervised learning, explaining the difference between regression and classification, showing how to evaluate and compare Machine Learning models and formalizing the concept of learning in general.
When taking our DL4NLP course, you do not necessarily have to re-watch all of the videos if you already have proficient knowledge in this area.</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/01_introduction/">Chapter 1: Introduction to the course</a>
    
      
        <p>In this chapter, you will dive into the fundamental principles of Deep Learning for Natural Language Processing (NLP). Explore key concepts including learning paradigms, various tasks within NLP, the neural probabilistic language model, and the significance of embeddings.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/02_dl_basics/">Chapter 2: Deep Learning Basics</a>
    
      
        <p>In this chapter we explore fundamental concepts like Recurrent Neural Networks (RNNs), the attention mechanism, ELMo embeddings, and tokenization. Each concept serves as a building block in understanding how neural networks can comprehend and generate human language.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/03_transformer/">Chapter 3: Transformer</a>
    
      
        <p>The Transformer, as introduced in [1], is a deep learning model architecture specifically designed for sequence-to-sequence tasks in natural language processing. It revolutionizes NLP by replacing recurrent layers with self-attention mechanisms, enabling it to process entire sequences in parallel, overcoming the limitations of sequential processing in traditional RNN-based models like LSTMs. This architecture has become the foundation for state-of-the-art models in various NLP tasks such as machine translation, text summarization, and language understanding. In this chapter we first introduce the transformer, explore different parts of it (Encoder and Decoder) and finally discuss ways to improve the architecture, such as Transformer-XL and Efficient Transformers.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_bert/">Chapter 4: BERT</a>
    
      
        <p>BERT (Bidirectional Encoder Representations from Transformers) [1] is a transformer-based model, designed to generate deep contextualized representations of words by considering bidirectional context, allowing it to capture complex linguistic patterns and context-dependent meanings. It achieves this by pretraining on large text corpora using masked language modeling and next sentence prediction objectives, enabling it to learn rich representations of words that incorporate both left and right context information.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/05_bert_based/">Chapter 5: Post-BERT Era</a>
    
      
        <p>Creating BERT-based models with modifications to pretraining involves adjusting the pretraining objectives or architecture to suit specific tasks or domains.This process typically begins by designing custom pre-training objectives or modifying existing ones to capture domain-specific characteristics or improve model performance on targeted tasks. These modified pre-training objectives can include variations of masked language modeling (MLM), next sentence prediction (NSP), or other self-supervised learning tasks tailored to the needs of the target domain. After pretraining, the model is fine-tuned on downstream tasks using task-specific data and objectives, enabling it to adapt its learned representations to the specific requirements of the tasks. In this chapter you will learn about three different cases where the existing BERT model has been modified, namely RoBERTa [1], ALBERT [2] and DistillBERT [3].
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/06_post_bert_t5/">Chapter 6: Post-BERT Era 2 and using the Transformer </a>
    
      
        <p>Here we further introduce models from the Post-BERT era, such as ELECTRA and XLNet. We also discuss how we can reformulate every task into a text-to-text format and finally introduce the T5 model.</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/07_gpt/">Chapter 7: Generative Pre-Trained Transformers</a>
    
      
        <p>In this chapter we will walk you through the history of the GPT models. Starting with GPT-1, then introducting GPT-2 and finally concluding with GPT-3.</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/08_llm/">Chapter 8: Large Language Models (LLMs)</a>
    
      
        <p>Here we cover Large Language Models and concepts, such as Instruction Fine-Tuning and Chain-of-thought Prompting.</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/09_rlhf/">Chapter 9: Reinforcement Learning from Human Feedback (RLHF)</a>
    
      
        <p>Here we cover the basics of RLHF and its related application.</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://slds-lmu.github.io/i2ml/" target="_blank">I2ML Course</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_dl4nlp" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/dl4nlp" target="_blank">Website source code</a></li>
  
</ul>
</footer>



</body>
</html>
