<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/dl4nlp/css/style.css">


<title>Deep Learning for Natural Language Processing (DL4NLP) | Chapters</title>


<link rel="apple-touch-icon" sizes="180x180" href="/dl4nlp/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/dl4nlp/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/dl4nlp/favicon-16x16.png">
<link rel="manifest" href="/dl4nlp/site.webmanifest">
<link rel="mask-icon" href="/dl4nlp/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/dl4nlp/dl4nlp.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/dl4nlp">
        
        Home
        </a>
        
        <a class="nav-link" href="/dl4nlp/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/dl4nlp/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/dl4nlp/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/dl4nlp/references/">
        
        References
        </a>
        
        <a class="nav-link" href="/dl4nlp/team/">
        
        Team
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapters</h1>

<p></p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/dl4nlp/chapters/00_basics/">Chapter 0: Machine Learning Basics</a>
    
      
        <p>This chapter introduces the basic concepts of Machine Learning. We therefore rely the excellent material from the I2ML Course which already comes with videos and has been taught LMU numerous times already.
The focus of these chapters in on introducing supervised learning, explaining the difference between regression and classification, showing how to evaluate and compare Machine Learning models and formalizing the concept of learning in general.
When taking our DL4NLP course, you do not necessarily have to re-watch all of the videos if you already have proficient knowledge in this area.</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/01_introduction/">Chapter 1: Introduction to the course</a>
    
      
        <p>This chapter gives a quick introduction to the basic concepts of Deep Learning for NLP, such as learning paradigms, tasks, neural probalistic language model and embeddings. A comprehensive introduction is given in [1].
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/02_dl_basics/">Chapter 2: Deep Learning Basics</a>
    
      
        <p>This chapter gives a quick introduction to the basic concepts of deep learning in the context of NLP, such as RNN, attention, ELMo and tokenization.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/03_advanced_nn/">Chapter 3: Transformer</a>
    
      
        <p>This chapter will introduce the Transformer architecture as introduced in [1]. We explore the different parts of the transformer model (Encoder and Decoder) and discuss ways to improve the architecture, such as Transformer-XL and Efficient Transformers.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/04_transfer_learning/">Chapter 4: Transfer Learning</a>
    
      
        <p>The term Transfer Learning generally refers to a special paradigm in machine learning, where the knowledge gained by an architecture while being trained on data from one task, domain or language is used to benefit training of the very same model on another task, domain or language. In the context of NLP, this mostly encapsulates pre-training on (unlabeled) general domain data in a self-supervised fashion and fine-tuning on (labaled) data from a specific task, domain or language.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/05_transformer/">Chapter 5: Transformer</a>
    
      
        <p>This chapter introduces a novel type of neural network architecture which nowadays is used as the backbone of nearly all state-of-the-art NLP models. It has nearly entirely replaced recurrent networks as the workhorse of the whole field and is currently very well on track to do the same to convolutional nets in Computer Vision as well.
</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/06_using_transformer/">Chapter 6: Using the Transformer</a>
    
      
        <p>Following the publication of the paper introducing the Transformer in late 2017, a multitude of researche rapdidly made use of (parts of) this architecture for transfer learning. We center this chapter around the (still) probably most prominent one, BERT, dividing the chapter in pre- and post-BERT as well as BERT based, while also differentiating with respect to which part of the transformer architecture was used by the respective model.</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/07_few_zero_shot/">Chapter 7: Few-/Zero-Shot Learning &amp; Prompting</a>
    
      
        <p>A concurring paradigm to the pretrain/finetune procedure, which most of the previously introduced architectures are subject to, is few- and zero-shot learning. In this paradigm, a (large!) pre-trained architecture is not fine-tuned to specific task, but prompted with a task description and a few labeled examples (or none, in case of zero-shot) in order to predict the correct label for an instance simply due to its capabilites gained during pre-training.</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/08_multilingual/">Chapter 8: Multilinguality</a>
    
      
        <p>Multilingual applications such as machine translation, e.g. Google Translate, are ubiquitous and make our lives easier. Thus, in this chapter we discuss the topic multilingual natural language processing, with a focus on multilingual word embeddings and multilingual tranformers.</p>
      
      
</li>

<li>
    <a class="title" href="/dl4nlp/chapters/09_further/">Chapter 9: Further topics</a>
    
      
        <p>This chapter is intended as a living and continuously growing chapter, in which we will include interesting papers/topics/architectures which do not fit into the course structure or are still to be researched.</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://slds-lmu.github.io/i2ml/" target="_blank">I2ML Course</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_dl4nlp" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/dl4nlp" target="_blank">Website source code</a></li>
  
</ul>
</footer>



</body>
</html>
