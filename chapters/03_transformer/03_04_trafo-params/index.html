<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/dl4nlp/css/style.css">


<title>Deep Learning for Natural Language Processing (DL4NLP) | Chapter 03.04: Transformer Parameter Count</title>


<link rel="apple-touch-icon" sizes="180x180" href="/dl4nlp/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/dl4nlp/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/dl4nlp/favicon-16x16.png">
<link rel="manifest" href="/dl4nlp/site.webmanifest">
<link rel="mask-icon" href="/dl4nlp/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/dl4nlp/dl4nlp.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/dl4nlp">
        
        Home
        </a>
        
        <a class="nav-link" href="/dl4nlp/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/dl4nlp/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/dl4nlp/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/dl4nlp/references/">
        
        References
        </a>
        
        <a class="nav-link" href="/dl4nlp/team/">
        
        Team
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapter 03.04: Transformer Parameter Count</h1>
<p>This chapter deals with the number of parameters of the transformer. The parameter count of a transformer model refers to the total number of learnable parameters present in its architecture, which are distributed across various components of the model.
These components typically include:</p>
<ol>
<li><strong>Embedding Layers</strong>: Parameters associated with the input and output embeddings for tokens, which encode their semantic meanings.</li>
<li><strong>Encoder Layers</strong>: Parameters within each encoder layer, including those associated with self-attention mechanisms, position-wise feedforward networks, and layer normalization.</li>
<li><strong>Decoder Layers</strong>: Parameters within each decoder layer, including self-attention mechanisms, cross-attention mechanisms, position-wise feedforward networks, and layer normalization.</li>
<li><strong>Positional Encodings</strong>: Parameters used to encode positional information in the input sequences.</li>
</ol>
<p>The total parameter count of a transformer model is the sum of parameters from all these components, with variations depending on the specific architecture and hyperparameters chosen for the model.</p>
<!--
### Lecture video
<div class="embedded_video">
  <iframe src="https://www.youtube-nocookie.com/embed/TfrSKiOecWI" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen title="YouTube Video"></iframe>
</div>

-->
<h3 id="lecture-slides">Lecture Slides</h3>


<script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous"></script>




  
  
  <a href="https://github.com/slds-lmu/lecture_dl4nlp/raw/chapter11/slides/chapter03-transformer/slides-34-trafo-params.pdf">
    <button class="btn btn-primary" style="margin-bottom:3rem">
      Download &raquo;slides-34-trafo-params.pdf&laquo;
    </button>
  </a>


<h3 id="additional-resources">Additional Resources</h3>
<ul>
<li><a href="https://towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0">Blog about the Transformer Parameter Count</a></li>
</ul>


<ul class="section_skipper list-unstyled">

  <li id="next_in_section"><a class="btn btn-primary" href="https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/">&#xab; Chapter 03.03: The Decoder</a></li>


  <li id="prev_in_section"><a class="btn btn-primary" href="https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_trafo_xl/">Chapter 03.05: Long Sequences: Transformer-XL &#xbb;</a></li>

</ul>




        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://slds-lmu.github.io/i2ml/" target="_blank">I2ML Course</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_dl4nlp" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/dl4nlp" target="_blank">Website source code</a></li>
  
</ul>
</footer>



</body>
</html>
