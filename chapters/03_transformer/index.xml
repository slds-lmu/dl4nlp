<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 3: Transformer on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/</link><description>Recent content in Chapter 3: Transformer on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 03.01: A universal deep learning architecture</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_01_intro_trafo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_01_intro_trafo/</guid><description>&lt;p>Transformers have been adapted and applied to various domains and tasks in addition to traditional sequence-to-sequence tasks in NLP. This chapter mentions a few examples of models that apply the transformer architecture to various domains.
Examples include: Vision Transformer (ViT) [1]: Utilizes transformer architecture for image classification tasks, demonstrating competitive performance compared to convolutional neural networks (CNNs). CLIP [2]: A model that connects images and text through a unified embedding space, enabling tasks such as zero-shot image classification and image-text retrieval.&lt;/p></description></item><item><title>Chapter 03.02: The Encoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_02_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_02_encoder/</guid><description>&lt;p>The Encoder in a transformer model is responsible for processing the input sequence and generating contextualized representations of each token, capturing both local and global dependencies within the sequence. It achieves this by employing self-attention mechanisms, which allow each token to attend to all other tokens in the input sequence, enabling the model to capture relationships and dependencies between tokens regardless of their positions. Additionally, the encoder includes position-wise feedforward networks to further refine the representations and incorporate positional information.&lt;/p></description></item><item><title>Chapter 03.03: The Decoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/</guid><description>&lt;p>The Decoder in a transformer model is responsible for generating an output sequence based on the contextualized representations generated by the encoder, facilitating tasks such as sequence generation and machine translation. It achieves this by utilizing self-attention mechanisms, similar to the encoder, to capture dependencies within the input sequence and cross-attention mechanisms to attend to the Encoder-output, enabling the model to focus on relevant parts of the input during decoding. Additionally, the decoder includes position-wise feedforward networks to further refine the representations and generate the output sequence token by token.&lt;/p></description></item><item><title>Chapter 03.04: Transformer Parameter Count</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_04_trafo-params/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_04_trafo-params/</guid><description>&lt;p>This chapter deals with the number of parameters of the transformer. The parameter count of a transformer model refers to the total number of learnable parameters present in its architecture, which are distributed across various components of the model.
These components typically include:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Embedding Layers&lt;/strong>: Parameters associated with the input and output embeddings for tokens, which encode their semantic meanings.&lt;/li>
&lt;li>&lt;strong>Encoder Layers&lt;/strong>: Parameters within each encoder layer, including those associated with self-attention mechanisms, position-wise feedforward networks, and layer normalization.&lt;/li>
&lt;li>&lt;strong>Decoder Layers&lt;/strong>: Parameters within each decoder layer, including self-attention mechanisms, cross-attention mechanisms, position-wise feedforward networks, and layer normalization.&lt;/li>
&lt;li>&lt;strong>Positional Encodings&lt;/strong>: Parameters used to encode positional information in the input sequences.&lt;/li>
&lt;/ol>
&lt;p>The total parameter count of a transformer model is the sum of parameters from all these components, with variations depending on the specific architecture and hyperparameters chosen for the model.&lt;/p></description></item><item><title>Chapter 03.05: Long Sequences: Transformer-XL</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_trafo_xl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_trafo_xl/</guid><description>&lt;p>This chapter is about the Transformer-XL [1] and how it deals with the issue of long sequences. Transformer-XL is an extension of the original Transformer architecture designed to address the limitations of long-range dependency modeling in sequence-to-sequence tasks. It aims to solve the problem of capturing and retaining information over long sequences by introducing a segment-level recurrence mechanism, enabling the model to process sequences of arbitrary length without being constrained by fixed-length contexts or running into computational limitations. Additionally, Transformer-XL incorporates relative positional embeddings to better capture positional information across segments of varying lengths.&lt;/p></description></item><item><title>Chapter 03.06: Efficient Transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_06_efficient/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_06_efficient/</guid><description>&lt;p>Efficient Transformers are designed to mitigate the computational and memory requirements of standard transformer architectures, particularly when dealing with large-scale datasets or resource-constrained environments. They aim to address issues such as scalability and efficiency in training and inference. One approach used in efficient transformers is replacing the standard self-attention mechanism with more lightweight attention mechanisms, which reduce the computational complexity of attending to long sequences by approximating the attention mechanism with lower-rank matrices or restricting attention to local or sparse regions of the sequence. These approaches enable transformers to be more practical for real-world applications where computational resources are limited.&lt;/p></description></item></channel></rss>