<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 3: Transformer on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/</link><description>Recent content in Chapter 3: Transformer on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 3.1: A universal deep learning architecture</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_01_intro_trafo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_01_intro_trafo/</guid><description>&lt;p>This chapter briefly introduces different use cases of the Transformer.&lt;/p></description></item><item><title>Chapter 3.2: The Encoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_02_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_02_encoder/</guid><description>&lt;p>This chapter further elaborates on the Transformer by focusing on the Encoder part and introducing the concepts of self- and cross attention.&lt;/p></description></item><item><title>Chapter 3.3: The Decoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/</guid><description>&lt;p>This chapter is about the decoder part of the transformer and masked self attention.&lt;/p></description></item><item><title>Chapter 3.4: Long Sequences: Transformer-XL</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_04_trafo_xl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_04_trafo_xl/</guid><description>&lt;p>This chapter is about the Transformer-XL and how it deals with the issue of long sequences.&lt;/p></description></item><item><title>Chapter 3.5: Efficient Transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_efficient/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_efficient/</guid><description>&lt;p>This chapter discusses the efficiency problems and shortcomings of transformer-based models and briefly talks about ways to deal with these issues.&lt;/p></description></item></channel></rss>