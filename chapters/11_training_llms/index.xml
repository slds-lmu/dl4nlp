<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 11: Training Large Language Models on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/</link><description>Recent content in Chapter 11: Training Large Language Models on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 11.01: Memory and compute requirements</title><link>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_01_compute_memory/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_01_compute_memory/</guid><description>&lt;p>Large language models (LLMs) require significant compute and memory resources due to their vast number of parameters and complex architectures. In this chapter you will learn about different contributions to compute requirements and how model size components influence memory requirements.&lt;/p></description></item><item><title>Chapter 11.02: How to reduce memory and compute?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_02_reduce_comp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_02_reduce_comp/</guid><description>&lt;p>Here you will learn about ways to reduce the memory and compute requirements for big models. We introduce distributed training, where you make use of data- and tensor parallellism, and FlashAttention, a method to perform attention more efficiently.&lt;/p></description></item><item><title>Chapter 11.03: Scaling Laws and Chinchilla</title><link>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_03_scaling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_03_scaling/</guid><description>&lt;p>In this chapter we introduce various scaling laws and chinchilla.&lt;/p></description></item><item><title>Chapter 11.04: LLM Optimization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_04_x_optimize/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_04_x_optimize/</guid><description>&lt;p>In this Chapter we discuss ways to optimize the performance of Large Language Models (LLMs) with methods such as Prompt engineering or methods beyond that.&lt;/p></description></item></channel></rss>