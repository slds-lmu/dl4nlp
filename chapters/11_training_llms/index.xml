<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 11: Training Large Language Models on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/</link><description>Recent content in Chapter 11: Training Large Language Models on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 11.01: LLMs: Parameters, Data, Hardware, Scaling</title><link>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_01_compute_scaling_chinchilla/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_01_compute_scaling_chinchilla/</guid><description>In this chapter you will learn how to calculate the number of parameters in the Transformer, understand Transformer computation and memory load, learn about Flash Attentions and understand Scaling Laws and Chinchilla.
Lecture Slides Download &amp;raquo;111-compute_scaling_chinchilla.pdf&amp;laquo;</description></item><item><title>Chapter 11.02: LLM Optimization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_02_x_optimize/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_02_x_optimize/</guid><description>In this Chapter we discuss ways to optimize the performance of Large Language Models (LLMs) with methods such as Prompt engineering or methods beyond that.
Lecture Slides Download &amp;raquo;112-slides-x-optimize.pdf&amp;laquo; Additional Resources Video by OpenAI about LLM Optimization</description></item></channel></rss>