<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning for NLP (DL4NLP) on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/</link><description>Recent content in Deep Learning for NLP (DL4NLP) on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 1.1: ML Basics (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-01-ml-basics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-01-ml-basics/</guid><description>&lt;p>This chapter introduces the basic concepts of Machine Learning. We focus on supervised learning, explain the difference between regression and classification, show how to evaluate and compare Machine Learning models and formalize the concept of learning.&lt;/p></description></item><item><title>Chapter 1.2: Supervised Regression (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-02-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-02-regression/</guid><description>&lt;p>This chapter treats the supervised regression task in more detail. We will see different loss functions for regression, how a linear regression model can be used from a Machine Learning perspective, and how to extend it with polynomials for greater flexibility.&lt;/p></description></item><item><title>Chapter 1.3: Supervised Classification (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-03-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-03-classification/</guid><description>&lt;p>This chapter treats the supervised classification task in more detail. We will see examples of binary and multiclass classification and the differences between discriminative and generative approaches. In particular, we will address logistic regression, discriminant analysis and naive Bayes classifiers.&lt;/p></description></item><item><title>Chapter 1.4: Multiclass Classification (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-04-multiclass/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-04-multiclass/</guid><description>&lt;p>This chapter treats the multiclass case of classification. Tasks with more than two classes preclude the application of some techniques studied in the binary scenario and require an adaptation of loss functions.&lt;/p></description></item><item><title>Chapter 1.5: Evaluation (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-05-evaluation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-05-evaluation/</guid><description>&lt;p>This chapter treats the challenge of evaluating the performance of a model. We will introduce different performance measures for regression and classification tasks, explain the problem of overfitting as well as the difference between training and test error, and, lastly, present a variety of resampling techniques.&lt;/p></description></item><item><title>Chapter 1.6: NLP tasks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-06-nlptasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-06-nlptasks/</guid><description>&lt;p>tbd&lt;/p></description></item><item><title>Chapter 1.7: Python</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-07-python/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01-07-python/</guid><description>&lt;p>Python is probably the most important programming language when you intend to work in the field of NLP. Thus, we provide a &lt;em>very basic&lt;/em> intro on Python and on how to set
up a project. Please note, that this will not be something like a programming course, but just the pure basics of how to set things up.&lt;/p></description></item><item><title>Chapter 02.01: Loss Functions for Regression</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_01_xyz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_01_xyz/</guid><description>&lt;p>\(L1\) and \(L2\) are two essential loss functions used for evaluating the performance of regression models. This section defines \(L1\) and \(L2\) loss and explains the differences.&lt;/p></description></item><item><title>Chapter 3.1: (Advanced) Tokenization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_advanced_nn/03_01_tokenization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_advanced_nn/03_01_tokenization/</guid><description>&lt;p>This chapter is about the different types of text tokenization used in NLP tasks.&lt;/p></description></item><item><title>Chapter 3.2: Word Embeddings</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_advanced_nn/03_02_embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_advanced_nn/03_02_embeddings/</guid><description>&lt;p>This chapter is about the concept of word embeddings and the different methods for creating them.&lt;/p></description></item><item><title>Chapter 3.3: Convolutional Networks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_advanced_nn/03_03_cnns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_advanced_nn/03_03_cnns/</guid><description>&lt;p>This chapter is about Convolutional Neural Networks and their applications.&lt;/p></description></item><item><title>Chapter 3.4: Recurrent Networks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_advanced_nn/03_04_rnns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_advanced_nn/03_04_rnns/</guid><description>&lt;p>This chapter is about Recurrent Neural Networks and their applications.&lt;/p></description></item><item><title>Chapter 3.5: Attention</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_advanced_nn/03_05_attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_advanced_nn/03_05_attention/</guid><description>&lt;p>This chapter is about attention mechanism and its application in neural networks.&lt;/p></description></item><item><title>Chapter 4.1: Challenges</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_00_challenges/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_00_challenges/</guid><description>&lt;p>blabla&lt;/p></description></item><item><title>Chapter 4.1.1: Low-resource environments</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_01_low_resource/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_01_low_resource/</guid><description>&lt;p>blabla&lt;/p></description></item><item><title>Chapter 4.2.2: Cross-lingual transfer</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_02_crosslingual/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_02_crosslingual/</guid><description>&lt;p>blabla&lt;/p></description></item><item><title>Chapter 4.1.3: Challenging tasks and data sets</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_03_tasks_datasets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_03_tasks_datasets/</guid><description>&lt;p>blabla&lt;/p></description></item><item><title>Chapter 4.1.4: Self-supervised pre-training</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_04_selfsup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_04_selfsup/</guid><description>&lt;p>blabla&lt;/p></description></item><item><title>Chapter 4.2: Data-driven tokenization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_02_00_datadriven/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_02_00_datadriven/</guid><description>&lt;p>blabla&lt;/p></description></item><item><title>Chapter 4.2.1: WordPiece tokenization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_02_01_wordpiece/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_02_01_wordpiece/</guid><description>&lt;p>blabla&lt;/p></description></item><item><title>Chapter 4.2.2: SentencePiece</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_02_02_sentencepiece/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_02_02_sentencepiece/</guid><description>&lt;p>blabla&lt;/p></description></item><item><title>Chapter 4.3: Early Transfer Learning architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_03_00_early_transfer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_03_00_early_transfer/</guid><description>&lt;p>Before the use of the Transformer architecture (&lt;a href="https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/">cf. Chapter 5&lt;/a>) for Transfer Learning, two milestone architectures were published.&lt;/p></description></item><item><title>Chapter 4.3.1: ELMo</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_03_01_elmo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_03_01_elmo/</guid><description>&lt;p>In their paper &lt;em>Deep contextualized word representations&lt;/em>, Peters et al. introduced a new class of word embeddings called ELMo (&lt;em>E&lt;/em>mbeddings from &lt;em>L&lt;/em>anguage &lt;em>Mo&lt;/em>dels).
Their unique feature is the ability to contextualize the embeddings in a bidirectional fashion (using biLSTMs), i.e. the representation of a word depends on the context on both sides. In order to obtain embeddings, whole sentences have to be fed to this architecture and the retrieved embeddings can then be used for a specific downstream task.&lt;/p></description></item><item><title>Chapter 4.3.2: ULMFiT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_03_02_ulmfit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_03_02_ulmfit/</guid><description>&lt;p>In contrast to ELMo, this architecture is not able to produce bidirectionally contextual embeddings but only unidirectionally contextual ones (since unidirectional LSTM are employed). Apart from this, is relies on a different transfer learning paradigm. Instead of &amp;ldquo;only&amp;rdquo; generating embedding which can be used for downstream tasks, the whole architecture is &lt;em>fine-tuned&lt;/em> towards a specific task. Hence the name &lt;em>U&lt;/em>niversal &lt;em>L&lt;/em>anguage &lt;em>M&lt;/em>odel &lt;em>Fi&lt;/em>ne-&lt;em>T&lt;/em>uning (ULMFiT).&lt;/p></description></item><item><title>Chapter 5.1: Byte-Pair Encoding (BPE)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_01_bpe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_01_bpe/</guid><description>&lt;p>Understanding BPE is crucial for understanding the transformer architecture, since it is used to learn the vocabulary for the embedding layer.&lt;br>
Instead of using a simple heuristic (characters or words), the tokens which constitute the vocabulary are formed base on the training corpus.&lt;/p></description></item><item><title>Chapter 5.2: Transformer Encoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_02_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_02_encoder/</guid><description>&lt;p>Since the initial use case was machine translation (cf. &lt;a href="../05_04_use_case">Chapter 5.4&lt;/a>), the transformer is an encoder-decoder architecture.
This chapter will explain the inner workings of the encoder part.&lt;/p></description></item><item><title>Chapter 5.3: Transformer Decoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_03_decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_03_decoder/</guid><description>&lt;p>This chapter will explain the inner workings of the decoder part of the transformer, especially the subtleties of Masked Self Attention and the Cross-Attention between encoder and decoder.&lt;/p></description></item><item><title>Chapter 5.4: Initial Use of the Transformer</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_04_use_case/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_04_use_case/</guid><description>&lt;p>In order to provide more context to get a better understanding of these model developments, we will quickly sketch the initial use of the Transformer for Machine Translation.&lt;/p></description></item><item><title>Chapter 5.5: Transformer-XL</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_05_trafo_xl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_05_trafo_xl/</guid><description>&lt;p>The Transfomer-XL is motivated by the problem of modeling long sequences with the Transformer architecture. While the computational complexity of the Vanilla Transformer
scales quadratically with the sequence length (and is this problematic), the authors here propose a segment recurrence mechanism to handle this problem efficiently.
Further the introduce &lt;em>relative&lt;/em> positional encodings (as opposed to absolute ones in the Vanilla Tranformer) to make the segment recurrence work in a meaningful way.&lt;/p></description></item><item><title>Chapter 6.1: Transformer Decoder - GPT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_01_gpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_01_gpt/</guid><description>&lt;p>This chapter will explain how the decoder part of the transformer was first used as the backbone for pre-training a language model. Radford et al. (2018) pre-trained a multi-layer transformer decoder using the language modeling objective on a large unannotated corpus (BooksCorpus; Zhu et al., 2015).&lt;/p></description></item><item><title>Chapter 6.2: Transformer Encoder - BERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_00_bert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_00_bert/</guid><description>&lt;p>This chapter will explain how the encoder part of the transformer was first used as the backbone for pre-training a language model. Devlin et al. (2019) came
up with a new objective, since the unconstrained self-attention in the encoder makes it impossible to train an encoder-based model on the language modeling objective
in a meaningful way. The resulting model (BERT) was first published as a pre-print in 10/2018 and presented at the ACL conference in 06/2019.&lt;/p></description></item><item><title>Chapter 6.2.1: BERT - Architectural details</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_01_architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_01_architecture/</guid><description>&lt;p>This chapter will explain the architectural details of BERT - from the (context-free) embedding layer until the final classification layer.&lt;/p></description></item><item><title>Chapter 6.2.2: BERT - Pre-training regime</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_02_pretrain/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_02_pretrain/</guid><description>&lt;p>This chapter will explain how BERT was pre-trained.&lt;/p></description></item><item><title>Chapter 6.2.3: BERT - Fine-tuning procedure</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_03_finetune/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_03_finetune/</guid><description>&lt;p>This chapter will explain how BERT can be fine-tuned on a specific task using labeled training data.&lt;/p></description></item><item><title>Chapter 6.2.4: BERT - Shortcomings &amp; Critique</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_04_short/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_04_short/</guid><description>&lt;p>This chapter will explain the shortcomings of BERT and explain the critique which this approach faces.&lt;/p></description></item><item><title>Chapter 6.2.5: BERT - Implications for future work</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_05_implications/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_05_implications/</guid><description>&lt;p>This chapter will address the learnings and the implications for future research which researchers took away from analyzing BERT.&lt;/p></description></item><item><title>Chapter 6.3: BERT-based architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_00_bert_based/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_00_bert_based/</guid><description>&lt;p>Following the advent of BERT, a great focus in this research are was on extending, explaining or improving it.
This chapter will explain three of the most important architectures from this streamline of reseach called BERTology&lt;/p></description></item><item><title>Chapter 6.3.1: RoBERTa</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_01_roberta/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_01_roberta/</guid><description>&lt;p>This chapter is about RoBERTa, an optimized version of the initial BERT architecture.&lt;/p></description></item><item><title>Chapter 6.3.2: ALBERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_02_albert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_02_albert/</guid><description>&lt;p>This chapter is about ALBERT, a more efficient version of the initial BERT architecture du to the application of parameter sharing techniques.&lt;/p></description></item><item><title>Chapter 6.3.3: Model distillation - DistilBERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_03_distilbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_03_distilbert/</guid><description>&lt;p>This chapter is about DistilBERT, an architecture derived from the original BERT model via model distillation.
We will explain the process of model distillation itself as well as the created architecture.&lt;/p></description></item><item><title>Chapter 6.4: Post-BERT architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_00_postbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_00_postbert/</guid><description>&lt;p>blabla&lt;/p></description></item><item><title>Chapter 6.4.1: Encoder architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_01_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_01_encoder/</guid><description>&lt;p>This chapter will cover a set of transformer-encoder based archtiectures like ELECTRA, ERNIE or BigBird.&lt;/p></description></item><item><title>Chapter 6.4.2: Decoder architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_02_decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_02_decoder/</guid><description>&lt;h2 id="this-chapter-will-cover-a-set-of-transformer-decoder-based-archtiectures-first-and-foremost-gpts-two-successors-and-the-xlnet-architecture-and-its-premutation-language-modeling-objective">This chapter will cover a set of transformer-decoder based archtiectures, first and foremost GPT&amp;rsquo;s two successors and the XLNet architecture and its premutation language modeling objective.&lt;/h2></description></item><item><title>Chapter 6.4.3: Encoder-Decoder architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_03_encdec/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_03_encdec/</guid><description>&lt;p>This chapter will cover a set of models using the whole transformer architecture again.&lt;/p></description></item><item><title>Chapter 7.1: What is ML?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_01_xyz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_01_xyz/</guid><description>&lt;p>As subtopic of artificial intelligence, machine learning is a mathematically well-defined discipline and usually constructs predictive or decision models from data, instead of explicitly programming them. In this section, you will see some typical examples of where machine learning is applied and the main directions of machine learning.&lt;/p></description></item><item><title>Chapter 8.1: Why Multilinguality?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_01_why_multilinguality/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_01_why_multilinguality/</guid><description>&lt;p>So far, we have seen interesting use cases for NLP. Clearly, these can be extended to multiple languages. But there are also interesting problems across languages. In this part we present a general motivation and a classification of problem types.&lt;/p></description></item><item><title>Chapter 8.2: Cross-lingual Word Embeddings</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_02_cross_lingual_word_embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_02_cross_lingual_word_embeddings/</guid><description>&lt;p>In this session, multilingual word embeddings are discussed. We describe the two main training strategies and look at examples of models to train those. Additionally, we look at unsupervised learning of multi-lingual word embeddings.&lt;/p></description></item><item><title>Chapter 9.1: What is ML?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/09_further/09_01_xyz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/09_further/09_01_xyz/</guid><description>&lt;p>As subtopic of artificial intelligence, machine learning is a mathematically well-defined discipline and usually constructs predictive or decision models from data, instead of explicitly programming them. In this section, you will see some typical examples of where machine learning is applied and the main directions of machine learning.&lt;/p></description></item><item><title/><link>https://slds-lmu.github.io/dl4nlp/exercises/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/exercises/</guid><description>Exercises Exercise Chapter 1 Exercise Chapter 2 Exercise Chapter 3 Exercise Chapter 4 Exercise Chapter 5 Exercise Chapter 6 Exercise Chapter 7 Exercise Chapter 8 Exercise Chapter 9 Exercise Chapter 10</description></item><item><title/><link>https://slds-lmu.github.io/dl4nlp/references/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/references/</guid><description>References Your markdown comes here!</description></item><item><title>Chapter 8.3: (Massively) Multilingual transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_03_multi_lingual_transformers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_03_multi_lingual_transformers/</guid><description>&lt;p>As we have previously seen, transformers are the working horse for modern NLP. In this section we learn how transformers are adapted to work in multilingual settings and discuss typical issues when training those. Furthermore we look at zero-shot cross lingual transfer capabilities.&lt;/p></description></item><item><title>Cheat Sheets</title><link>https://slds-lmu.github.io/dl4nlp/appendix/01_cheat_sheets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/01_cheat_sheets/</guid><description>possible coming in the future ..</description></item><item><title>Errata</title><link>https://slds-lmu.github.io/dl4nlp/appendix/02_errata/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/02_errata/</guid><description>Errata in the slides shown in the videos to be added once videos + updated slides thereafter are available 😉</description></item><item><title>Related Courses</title><link>https://slds-lmu.github.io/dl4nlp/appendix/03_related/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/03_related/</guid><description>Other ML courses Introduction to Machine Learning (I2ML) Introduction to Deep Learning (I2DL)</description></item></channel></rss>