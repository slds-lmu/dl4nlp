<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning for NLP (DL4NLP) on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/</link><description>Recent content in Deep Learning for NLP (DL4NLP) on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 1.1: What is ML?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01_01_xyz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01_01_xyz/</guid><description>&lt;p>As subtopic of artificial intelligence, machine learning is a mathematically well-defined discipline and usually constructs predictive or decision models from data, instead of explicitly programming them. In this section, you will see some typical examples of where machine learning is applied and the main directions of machine learning.&lt;/p></description></item><item><title>Chapter 1.2: Data</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01_02_xyz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01_02_xyz/</guid><description>&lt;p>In this section we explain the basic structure of tabular data used in machine learning. We will differentiate targets from features, talk about labeled and unlabeled data and introduce the concept of the data generating process.&lt;/p></description></item><item><title>Chapter 1.3: Tasks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01_03_xyz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_basics/01_03_xyz/</guid><description>&lt;p>The tasks of supervised learning can roughly be divided in two categories: regression (for continuous outcome) and classification (for categorical outcome). We will present some examples.&lt;/p></description></item><item><title>Chapter 02.01: Loss Functions for Regression</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_01_xyz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_01_xyz/</guid><description>&lt;p>\(L1\) and \(L2\) are two essential loss functions used for evaluating the performance of regression models. This section defines \(L1\) and \(L2\) loss and explains the differences.&lt;/p></description></item><item><title>Chapter 3.1: What is ML?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_advanced_nn/03_01_xyz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_advanced_nn/03_01_xyz/</guid><description>&lt;p>As subtopic of artificial intelligence, machine learning is a mathematically well-defined discipline and usually constructs predictive or decision models from data, instead of explicitly programming them. In this section, you will see some typical examples of where machine learning is applied and the main directions of machine learning.&lt;/p></description></item><item><title>Chapter 4.1: What is ML?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_xyz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_xyz/</guid><description>&lt;p>As subtopic of artificial intelligence, machine learning is a mathematically well-defined discipline and usually constructs predictive or decision models from data, instead of explicitly programming them. In this section, you will see some typical examples of where machine learning is applied and the main directions of machine learning.&lt;/p></description></item><item><title>Chapter 5.1: Byte-Pair Encoding (BPE)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_01_bpe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_01_bpe/</guid><description>&lt;p>Understanding BPE is crucial for understanding the transformer architecture, since it is used to learn the vocabulary for the embedding layer.&lt;br>
Instead of using a simple heuristic (characters or words), the tokens which constitute the vocabulary are formed base on the training corpus.&lt;/p></description></item><item><title>Chapter 5.2: Transformer Encoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_02_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_02_encoder/</guid><description>&lt;p>Since the initial use case was machine translation (cf. &lt;a href="../05_04_use_case">Chapter 5.4&lt;/a>), the transformer is an encoder-decoder architecture.
This chapter will explain the inner workings of the encoder part.&lt;/p></description></item><item><title>Chapter 5.3: Transformer Decoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_03_decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_03_decoder/</guid><description>&lt;p>This chapter will explain the inner workings of the decoder part of the transformer, especially the subtleties of Masked Self Attention and the Cross-Attention between encoder and decoder.&lt;/p></description></item><item><title>Chapter 5.4: Initial Use of the Transformer</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_04_use_case/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_04_use_case/</guid><description>&lt;p>to be written&lt;/p></description></item><item><title>Chapter 5.5: Transformer-XL for long sequences</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_05_trafo_xl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_05_trafo_xl/</guid><description>&lt;p>to be written&lt;/p></description></item><item><title>Chapter 6.1: Transformer Decoder - GPT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_01_gpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_01_gpt/</guid><description>&lt;p>This chapter will explain how the decoder part of the transformer was first used as the backbone for pre-training a language model. Radford et al. (2018) pre-trained a multi-layer transformer decoder using the language modeling objective on a large unannotated corpus (BooksCorpus; Zhu et al., 2015).&lt;/p></description></item><item><title>Chapter 6.2: Transformer Encoder - BERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_bert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_bert/</guid><description>&lt;p>This chapter will explain how the encoder part of the transformer was first used as the backbone for pre-training a language model. Devlin et al. (2019) came
up with a new objective, since the unconstrained self-attention in the encoder makes it impossible to train an encoder-based model on the language modeling objective
in a meaningful way. The resulting model (BERT) was first published as a pre-print in 10/2018 and presented at the ACL conference in 06/2019.&lt;/p></description></item><item><title>Chapter 6.2.1: BERT - Architectural details</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_01_architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_01_architecture/</guid><description>&lt;p>This chapter will explain the arcitectural details of BERT - from the (context-free) embedding layer until the final classification layer.&lt;/p></description></item><item><title>Chapter 7.1: What is ML?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_01_xyz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_01_xyz/</guid><description>&lt;p>As subtopic of artificial intelligence, machine learning is a mathematically well-defined discipline and usually constructs predictive or decision models from data, instead of explicitly programming them. In this section, you will see some typical examples of where machine learning is applied and the main directions of machine learning.&lt;/p></description></item><item><title>Chapter 8.1: What is ML?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_01_xyz/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_01_xyz/</guid><description>&lt;p>As subtopic of artificial intelligence, machine learning is a mathematically well-defined discipline and usually constructs predictive or decision models from data, instead of explicitly programming them. In this section, you will see some typical examples of where machine learning is applied and the main directions of machine learning.&lt;/p></description></item><item><title/><link>https://slds-lmu.github.io/dl4nlp/exercises/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/exercises/</guid><description>Exercises Your markdown comes here!</description></item><item><title/><link>https://slds-lmu.github.io/dl4nlp/references/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/references/</guid><description>References Your markdown comes here!</description></item><item><title>Cheat Sheets</title><link>https://slds-lmu.github.io/dl4nlp/appendix/01_cheat_sheets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/01_cheat_sheets/</guid><description/></item><item><title>Errata</title><link>https://slds-lmu.github.io/dl4nlp/appendix/02_errata/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/02_errata/</guid><description>Errata in the slides shown in the videos Chapter 1.4 (Models &amp;amp; Parameters) - slide 5/10: d-dimensional vector, not p-dimensional Chapter 4.3 (Simple Measures for Classification) - slide 6/9: Error in cost matrix Chapter 5.2 (CART: Splitting Criteria) - slide 12/12: Error in result of Gini Chapter 6.2 (Forests: Intro) - slides 7/8 and 8/8: Error in OOB error Chapter 6.4 (Forests: Feature importance) - slide 3/3: Error in permutation based variable importance</description></item></channel></rss>