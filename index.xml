<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning for NLP (DL4NLP) on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/</link><description>Recent content in Deep Learning for NLP (DL4NLP) on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 0.1: ML Basics (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-01-ml-basics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-01-ml-basics/</guid><description>&lt;p>This chapter introduces the basic concepts of Machine Learning. We focus on supervised learning, explain the difference between regression and classification, show how to evaluate and compare Machine Learning models and formalize the concept of learning.&lt;/p></description></item><item><title>Chapter 0.2: Supervised Regression (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-02-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-02-regression/</guid><description>&lt;p>This chapter treats the supervised regression task in more detail. We will see different loss functions for regression, how a linear regression model can be used from a Machine Learning perspective, and how to extend it with polynomials for greater flexibility.&lt;/p></description></item><item><title>Chapter 0.3: Supervised Classification (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-03-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-03-classification/</guid><description>&lt;p>This chapter treats the supervised classification task in more detail. We will see examples of binary and multiclass classification and the differences between discriminative and generative approaches. In particular, we will address logistic regression, discriminant analysis and naive Bayes classifiers.&lt;/p></description></item><item><title>Chapter 0.4: Multiclass Classification (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-04-multiclass/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-04-multiclass/</guid><description>&lt;p>This chapter treats the multiclass case of classification. Tasks with more than two classes preclude the application of some techniques studied in the binary scenario and require an adaptation of loss functions.&lt;/p></description></item><item><title>Chapter 0.5: Evaluation (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-05-evaluation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-05-evaluation/</guid><description>&lt;p>This chapter treats the challenge of evaluating the performance of a model. We will introduce different performance measures for regression and classification tasks, explain the problem of overfitting as well as the difference between training and test error, and, lastly, present a variety of resampling techniques.&lt;/p></description></item><item><title>Chapter 10.01: Why Multilinguality?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/10_multilingual/10_01_why_multilinguality/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/10_multilingual/10_01_why_multilinguality/</guid><description>We need multilingual models to bridge language barriers, enhance global communication, and ensure equitable access to information and technology across diverse linguistic communities. These models enable seamless translation, cross-lingual information retrieval, and multi-language support in applications, allowing people to interact with technology in their native languages. By addressing the challenges of linguistic diversity, multilingual models promote inclusivity, facilitate international collaboration, and democratize access to digital resources and services globally.
Lecture Slides Download &amp;raquo;slides-101-why_multilingual.</description></item><item><title>Chapter 10.02: Cross-lingual Word Embeddings</title><link>https://slds-lmu.github.io/dl4nlp/chapters/10_multilingual/10_02_cross_lingual_word_embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/10_multilingual/10_02_cross_lingual_word_embeddings/</guid><description>Cross-lingual word embeddings create a shared vector space for words from multiple languages, allowing models to understand and process text across different languages seamlessly. In this chapter we describe the two main training strategies and look at examples of models to train those. Additionally, we look at unsupervised learning of multi-lingual word embeddings.
Lecture Slides Download &amp;raquo;slides-102-multilingual-wordembs.pdf&amp;laquo;</description></item><item><title>Chapter 10.03: (Massively) Multilingual Transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/10_multilingual/10_03_multi_lingual_transformers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/10_multilingual/10_03_multi_lingual_transformers/</guid><description>As we have previously seen, transformers are the working horse for modern NLP. In this section we learn how transformers are adapted to work in multilingual settings and discuss typical issues when training those. Furthermore we look at zero-shot cross lingual transfer capabilities.
Lecture Slides Download &amp;raquo;slides-102-multilingual-wordembs.pdf&amp;laquo;</description></item><item><title>Chapter 01.01: Introduction and Course Outline</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_01_course_intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_01_course_intro/</guid><description>&lt;p>This chapter introduces the people responsible for the course, aims to answer all open question and should give an impression of the expected workload.&lt;/p></description></item><item><title>Chapter 01.02: Learning Paradigms</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_02_learningparadigms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_02_learningparadigms/</guid><description>&lt;p>This chapter introduces different learning paradigms, such as embeddings, prompting and pre-training and finetuning a model. These are all very important concepts in the context of Deep Learning for NLP. The purpose of this chapter is to give an overview over these concepts.&lt;/p></description></item><item><title>Chapter 01.03: NLP tasks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_03_tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_03_tasks/</guid><description>&lt;p>Here you will learn about the distinctions between low-level and high-level tasks, as well as the differences between linguistic tasks and broader classification tasks.&lt;/p></description></item><item><title>Chapter 01.04: Neural Probabalistic Language Model</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_04_nplm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_04_nplm/</guid><description>&lt;p>In this chapter, take your first steps into the world of Deep Neural Networks for language modeling with the Neural Probabilistic Language Model (NPLM) [1]. Explore how NPLM represents a foundational approach in utilizing deep learning techniques for understanding and generating natural language. Uncover the principles behind NPLM and its significance in paving the way for more sophisticated language models.&lt;/p></description></item><item><title>Chapter 01.05: Word Embeddings</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_05_embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_05_embeddings/</guid><description>&lt;p>Embeddings allow us to represent tokens with a vector representation, enabling computers to efficiently understand the meaning and context of different words. In this chapter we will first explain the general concept of embeddings and then introduce two popular approaches, namely Word2Vec [1] and FastText [2]. Word2vec is a neural network model that learns distributed representations of words in a continuous vector space based on their contextual usage in a corpus of text. Word2vec achieves this by training on a large dataset to predict the surrounding words given a target word, embedding each word as a dense vector where similar words are closer together in the vector space. FastText is an extension of Word2vec that represents words as bags of character n-grams, enabling it to capture morphological information alongside semantics. It accomplishes this by breaking down each word into character n-grams, creating embeddings for these subword units, and then summing or averaging these embeddings to obtain the representation for the whole word.&lt;/p></description></item><item><title>Chapter 11.01: LLMs: Parameters, Data, Hardware, Scaling</title><link>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_01_compute_scaling_chinchilla/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_01_compute_scaling_chinchilla/</guid><description>In this chapter you will learn how to calculate the number of parameters in the Transformer, understand Transformer computation and memory load, learn about Flash Attentions and understand Scaling Laws and Chinchilla.
Lecture Slides Download &amp;raquo;111-compute_scaling_chinchilla.pdf&amp;laquo;</description></item><item><title>Chapter 11.02: LLM Optimization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_02_x_optimize/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/11_training_llms/11_02_x_optimize/</guid><description>In this Chapter we discuss ways to optimize the performance of Large Language Models (LLMs) with methods such as Prompt engineering or methods beyond that.
Lecture Slides Download &amp;raquo;112-slides-x-optimize.pdf&amp;laquo; Additional Resources Video by OpenAI about LLM Optimization</description></item><item><title>Chapter 02.01: Recurrent Neural Networks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_01_rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_01_rnn/</guid><description>&lt;p>Conventional Feed Forward Neural Networks donâ€™t allow us to process sequential data, which is why we need Recurrent Neural Networks (RNNs) to handle text data. In this chapter we also get to know models that help us to overcome the limits of simple RNNs. You will learn about LSTMs and Bidirectional RNNs. LSTMs incorporate different gates that control the information flow of the network and allow us to model long term dependencies. Bidirectional RNNs introduce bidirectionality into the model, which allows it to not only learn from the left side but also from the right side context of sequential data&lt;/p></description></item><item><title>Chapter 02.02 Attention</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_02_attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_02_attention/</guid><description>&lt;p>This chapter will give you a first introduction into the concept of Attention, as introduced in [1]. Attention mechanisms allow neural networks to focus on specific parts of the input sequence, assigning varying degrees of importance to different elements, enhancing performance especially in tasks where long-range dependencies are crucial, overcoming limitations of LSTMs and vanilla bidirectional RNNs which struggle with retaining information across long sequences or capturing complex relationships between distant elements. This is achieved by dynamically weighting the importance of different parts of the input sequence during computation, enabling the model to attend to relevant information and effectively process inputs of varying lengths.&lt;/p></description></item><item><title>Chapter 02.03: ELMo</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_03_elmo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_03_elmo/</guid><description>&lt;p>Here you will learn about ELMo (Embeddings from Language Models) [1], which is a deep contextualized word representation model that generates word embeddings by considering the entire input sentence, capturing complex linguistic features and contextual nuances. It accomplishes this by using a bidirectional LSTM (Long Short-Term Memory) network to generate contextualized word representations, where each word&amp;rsquo;s embedding is dynamically influenced by its surrounding context. This enables ELMo embeddings to capture polysemy, syntactic variations, and semantic nuances that traditional word embeddings like Word2vec or FastText may miss.&lt;/p></description></item><item><title>Chapter 02.04 Revisiting words: Tokenization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_04_tokenization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_04_tokenization/</guid><description>&lt;p>This chapter is about Tokenization, which is the process of breaking down a sequence of text into smaller, meaningful units, such as words or subwords, to facilitate natural language processing tasks. Various tokenization methods exist, including Byte Pair Encoding (BPE) [1] or WordPiece [2], each with its own approach to dividing text into tokens. BPE and WordPiece are subword tokenization techniques that iteratively merge frequent character sequences to form larger units, effectively capturing both common words and rare morphological variations.&lt;/p></description></item><item><title>Chapter 03.01: A universal deep learning architecture</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_01_intro_trafo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_01_intro_trafo/</guid><description>&lt;p>Transformers have been adapted and applied to various domains and tasks in addition to traditional sequence-to-sequence tasks in NLP. This chapter mentions a few examples of models that apply the transformer architecture to various domains.
Examples include: Vision Transformer (ViT) [1]: Utilizes transformer architecture for image classification tasks, demonstrating competitive performance compared to convolutional neural networks (CNNs). CLIP [2]: A model that connects images and text through a unified embedding space, enabling tasks such as zero-shot image classification and image-text retrieval.&lt;/p></description></item><item><title>Chapter 03.02: The Encoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_02_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_02_encoder/</guid><description>&lt;p>The Encoder in a transformer model is responsible for processing the input sequence and generating contextualized representations of each token, capturing both local and global dependencies within the sequence. It achieves this by employing self-attention mechanisms, which allow each token to attend to all other tokens in the input sequence, enabling the model to capture relationships and dependencies between tokens regardless of their positions. Additionally, the encoder includes position-wise feedforward networks to further refine the representations and incorporate positional information.&lt;/p></description></item><item><title>Chapter 03.03: The Decoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/</guid><description>&lt;p>The Decoder in a transformer model is responsible for generating an output sequence based on the contextualized representations generated by the encoder, facilitating tasks such as sequence generation and machine translation. It achieves this by utilizing self-attention mechanisms, similar to the encoder, to capture dependencies within the input sequence and cross-attention mechanisms to attend to the Encoder-output, enabling the model to focus on relevant parts of the input during decoding. Additionally, the decoder includes position-wise feedforward networks to further refine the representations and generate the output sequence token by token.&lt;/p></description></item><item><title>Chapter 03.04: Long Sequences: Transformer-XL</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_04_trafo_xl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_04_trafo_xl/</guid><description>&lt;p>This chapter is about the Transformer-XL [1] and how it deals with the issue of long sequences. Transformer-XL is an extension of the original Transformer architecture designed to address the limitations of long-range dependency modeling in sequence-to-sequence tasks. It aims to solve the problem of capturing and retaining information over long sequences by introducing a segment-level recurrence mechanism, enabling the model to process sequences of arbitrary length without being constrained by fixed-length contexts or running into computational limitations. Additionally, Transformer-XL incorporates relative positional embeddings to better capture positional information across segments of varying lengths.&lt;/p></description></item><item><title>Chapter 03.05: Efficient Transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_efficient/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_efficient/</guid><description>&lt;p>Efficient Transformers are designed to mitigate the computational and memory requirements of standard transformer architectures, particularly when dealing with large-scale datasets or resource-constrained environments. They aim to address issues such as scalability and efficiency in training and inference. One approach used in efficient transformers is replacing the standard self-attention mechanism with more lightweight attention mechanisms, which reduce the computational complexity of attending to long sequences by approximating the attention mechanism with lower-rank matrices or restricting attention to local or sparse regions of the sequence. These approaches enable transformers to be more practical for real-world applications where computational resources are limited.&lt;/p></description></item><item><title>Chapter 04.01: ARLMs vs. MLM</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_01_arlm_mlm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_01_arlm_mlm/</guid><description>&lt;p>ARLM (Auto-Regressive Language Modeling) and MLM (Masked Language Modeling) are both self-supervised learning objectives used in pretraining transformer-based language models like BERT. ARLM involves predicting the next word in a sequence given the previous context, while MLM involves masking some of the input tokens and predicting them based on the surrounding context. Both methods leverage self-supervision, where the model learns from the data itself without requiring explicit labels, enabling it to capture meaningful representations of language.&lt;/p></description></item><item><title>Chapter 04.02: Measuring Performance</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_02_metrics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_02_metrics/</guid><description>&lt;p>Measuring the performance of language models poses challenges due to the subjective nature of language understanding and generation, as well as the diversity of tasks they are applied to.
Performance can be assessed through various metrics including:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Perplexity&lt;/strong>: Measures the model&amp;rsquo;s uncertainty in predicting the next word in a sequence, with lower perplexity indicating better performance.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Accuracy&lt;/strong>: Measures the proportion of correct predictions made by the model on a classification task.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>BLEU (Bilingual Evaluation Understudy)&lt;/strong>: Evaluates the quality of machine-translated text by comparing it to one or more reference translations.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)&lt;/strong>: Measures the overlap between system-generated summaries and reference summaries.
However, each metric has its limitations and may not fully capture the model&amp;rsquo;s performance across all tasks and domains, highlighting the difficulty of comprehensive evaluation in natural language processing.&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Chapter 04.03: The Architecture</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_03_corefacts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_03_corefacts/</guid><description>&lt;p>The architecture of BERT primarily revolves around transformer encoders, where stacked layers of self-attention mechanisms and feedforward neural networks are employed to generate contextualized representations of tokens in bidirectional context. During pre-training, BERT utilizes masked language modeling (MLM) and next sentence prediction (NSP) tasks to fine-tune the parameters of the transformer encoder layers, enabling the model to effectively capture semantic relationships and contextual nuances in text data.&lt;/p></description></item><item><title>Chapter 04.04: Pre-training &amp; Fine-Tuning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_04_pretrain_finetune/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_04_pretrain_finetune/</guid><description>&lt;p>In the pre-training phase of BERT, the model is trained on large text corpora using self-supervised learning objectives such as masked language modeling (MLM) and next sentence prediction (NSP). During MLM a certain percentage of input tokens are randomly masked and the model is trained to predict the masked tokens based on the surrounding context. In NSP the model learns to predict whether two sentences in a pair are consecutive or not. This pre-training phase allows BERT to learn rich contextual representations of words and sentences.
In the fine-tuning phase BERT is adapted to specific downstream tasks by adding task-specific output layers and fine-tuning the pretrained parameters on task-specific labeled data. During fine-tuning only a small portion of the parameters are updated, while the majority of the pretrained parameters remain fixed. This process allows BERT to leverage the general linguistic knowledge learned during pretraining and adapt it to the specific requirements of the downstream tasks, resulting in improved performance on tasks such as text classification, named entity recognition, and question answering.&lt;/p></description></item><item><title>Chapter 04.05: Transfer Learning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_05_transferlearning_selfsup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_05_transferlearning_selfsup/</guid><description>&lt;p>Transfer learning is a machine learning approach where knowledge acquired from solving one task is applied to a different but related task, typically using pretrained models. In the context of BERT, transfer learning involves leveraging the pretraining phase where the model learns general language representations on large text corpora, and then fine-tuning these representations on downstream tasks. This allows BERT to transfer the knowledge gained during pre-training to specific tasks, enabling it to achieve better performance with less labeled data compared to training from scratch.&lt;/p></description></item><item><title>Chapter 05.01: Implications for future work &amp; BERTology</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_01_bertology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_01_bertology/</guid><description>&lt;p>BERT (Bidirectional Encoder Representations from Transformers) has significantly impacted research in natural language processing (NLP) by introducing the concept of contextualized word embeddings and demonstrating the effectiveness of large-scale pretraining followed by fine-tuning on downstream tasks. Prior to BERT, models like Word2vec and fasttext generated static word embeddings that lacked context, limiting their ability to capture the nuances of language. BERT&amp;rsquo;s bidirectional approach to pretraining allowed it to capture rich contextual information, leading to substantial improvements in performance across a wide range of NLP tasks. Additionally, the widespread adoption of BERT sparked a new area of research known as &amp;ldquo;BERTology,&amp;rdquo; which focuses on understanding the inner workings of transformer-based models like BERT through empirical analysis, ablation studies, and probing experiments. This research has led to deeper insights into the mechanisms underlying these models and has inspired further innovations in model architectures, pre-training objectives, and fine-tuning strategies in NLP.&lt;/p></description></item><item><title>Chapter 05.02: BERT-based architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_02_bert_based/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_02_bert_based/</guid><description>&lt;p>For RoBERTa (Robustly optimized BERT approach), the basic idea is to improve upon the BERT architecture by training on more data with longer sequences, removing the next sentence prediction (NSP) objective, and utilizing dynamic masking during pretraining to enhance robustness and performance.
RoBERTa achieves this by pretraining on a larger corpus of text data with more training steps, larger batch sizes, and longer sequences compared to BERT. Additionally, it removes the NSP objective and employs dynamic masking, where masking patterns are dynamically sampled for each training instance, allowing the model to see different masked tokens across multiple epochs. This approach enhances the model&amp;rsquo;s ability to capture contextual information and improves performance on various natural language understanding tasks.&lt;/p>
&lt;p>As for ALBERT (A Lite BERT), the basic idea is to reduce the computational complexity of BERT while maintaining or improving performance by introducing parameter-sharing techniques and factorized embedding parameterization.
ALBERT achieves this by factorizing the embedding parameters and sharing them across layers, reducing the number of parameters and computational cost. These innovations enable ALBERT to achieve similar or better performance than BERT while being more memory-efficient and scalable, making it suitable for a wider range of applications and deployment scenarios.&lt;/p></description></item><item><title>Chapter 05.03: Model distillation</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_03_distilbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_03_distilbert/</guid><description>&lt;p>For model distillation, the basic idea is to transfer knowledge from a large, computationally expensive model (e.g., BERT) to a smaller, more efficient model (e.g., DistilBERT) by distilling the large model&amp;rsquo;s knowledge into the smaller one through a process called distillation.
DistilBERT, as an example, achieves this by first pretraining a large BERT model on a large corpus of text data using standard pre-training objectives. Then, the knowledge learned by the large model is transferred to a smaller, distilled model, such as DistilBERT, by training the smaller model to mimic the behavior of the larger model. This is typically done by minimizing the discrepancy between the probability distributions of the predictions made by the large model and the distilled model on the same input data.
During this process, unnecessary parameters and redundant information are discarded, resulting in a smaller and more efficient model while retaining much of the performance of the larger model. This allows for faster inference and deployment in resource-constrained environments without sacrificing much in terms of performance. DistilBERT, specifically, achieves a significant reduction in model size and computational cost compared to BERT while maintaining competitive performance across various NLP tasks.&lt;/p></description></item><item><title>Chapter 06.01: Post-BERT architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_01_postbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_01_postbert/</guid><description>&lt;p>This chapter will introduce two new architectures from the post-BERT era, namely ELECTRA [1] and XLNet [2]. By changing the pre training approach we can create new models. For ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately), the basic idea is to train a discriminator model to distinguish between the original tokens in a text sequence and replaced tokens generated by a generator model, aiming to create a more efficient pretraining approach compared to masked language modeling (MLM).
For XLNet, the basic idea is to overcome the limitations of unidirectional and bidirectional language models by introducing a permutation-based pre-training objective, the so called permutation language modeling (PLM), that enables the model to consider all possible permutations of the input tokens, capturing bidirectional context.&lt;/p></description></item><item><title>Chapter 06.02: Tasks as text-to-text problem</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_02_text2text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_02_text2text/</guid><description>&lt;p>Reformulating various NLP tasks as text-to-text tasks aims to simplify model architectures and improve performance by treating all tasks as instances of generating output text from input text.
This approach addresses shortcomings of BERT&amp;rsquo;s original design, where different tasks required different output layers and training objectives, leading to a complex multitask learning setup. By unifying tasks under a single text-to-text framework, models can be trained more efficiently and generalize better across diverse tasks and domains.&lt;/p></description></item><item><title>Chapter 06.03: Text-to-Text Transfer Transformer</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_03_t5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_03_t5/</guid><description>&lt;p>T5 (Text-To-Text Transfer Transformer) [1] aims to unify various natural language processing tasks by framing them all as text-to-text transformations, simplifying model architectures and enabling flexible training across diverse tasks.
It achieves this by formulating input-output pairs for different tasks as text sequences, allowing the model to learn to generate target text from source text regardless of the specific task, facilitating multitask learning and transfer learning across tasks with a single, unified architecture.&lt;/p></description></item><item><title>Chapter 07.01: GPT-1 (2018)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_01_gpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_01_gpt/</guid><description>&lt;p>GPT-1 [1] introduces a novel approach to natural language processing by employing a generative transformer architecture pre-trained on a vast corpus of text data, where task-specific input transformations are performed to adapt the model to different tasks.
By fine-tuning the model on task-specific data with minimal changes to the architecture, GPT-1 demonstrates the effectiveness of transfer learning and showcases the potential of generative transformers in a wide range of natural language understanding and generation tasks.&lt;/p></description></item><item><title>Chapter 07.02: GPT-2 (2019)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_02_gpt2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_02_gpt2/</guid><description>&lt;p>GPT-2 [1] builds upon its predecessor with a larger model size, more training data, and improved architecture. Like GPT-1, GPT-2 utilizes a generative transformer architecture but features a significantly increased number of parameters, leading to enhanced performance in language understanding and generation tasks. Additionally, GPT-2 introduces a scaled-up version of the training data and fine-tuning techniques to further refine its language capabilities.&lt;/p></description></item><item><title>Chapter 07.03: GPT-3 (2020) &amp; X-shot learning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_03_gpt3xshot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_03_gpt3xshot/</guid><description>&lt;p>In this chapter, we&amp;rsquo;ll explore GPT-3 [1]. GPT-3 builds on the successes of its predecessors, boasting a massive architecture and extensive pre-training on diverse text data. Unlike previous models, GPT-3 introduces a few-shot learning approach, allowing it to perform tasks with minimal task-specific training data. With its remarkable scale and versatility, GPT-3 represents a significant advancement in natural language processing, showcasing the potential of large-scale transformer architectures in various applications.&lt;/p></description></item><item><title>Chapter 07.04: Tasks &amp; Performance</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_04_tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_04_tasks/</guid><description>&lt;p>GPT-3 has X-shot abilities, meaning it is able to perform tasks with minimal or even no task-specific training data. This chapter provides an overview over various different tasks and illustrates the X-shot capabilities of GPT-3. Additionally you will be introduced to relevant benchmarks.&lt;/p></description></item><item><title>Chapter 07.05: Discussion: Ethics and Cost</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_05_discussion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_05_discussion/</guid><description>&lt;p>In discussing GPT-3&amp;rsquo;s ethical implications, it is crucial to consider its potential societal impact, including issues surrounding bias, misinformation, and data privacy. With its vast language generation capabilities, GPT-3 has the potential to disseminate misinformation at scale, posing risks to public trust and safety. Additionally, the model&amp;rsquo;s reliance on large-scale pretraining data raises concerns about reinforcing existing biases present in the data, perpetuating societal inequalities. Furthermore, the use of GPT-3 in sensitive applications such as content generation, automated customer service, and decision-making systems raises questions about accountability, transparency, and unintended consequences. As such, responsible deployment of GPT-3 requires careful consideration of ethical guidelines, regulatory frameworks, and robust mitigation strategies to address these challenges and ensure the model&amp;rsquo;s ethical use in society.&lt;/p></description></item><item><title>Chapter 08.01: Instruction Fine-Tuning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_01_instruction_tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_01_instruction_tuning/</guid><description>&lt;p>Instruction fine-tuning aims to enhance the adaptability of large language models (LLMs) by providing explicit instructions or task descriptions, enabling more precise control over model behavior and adaptation to diverse contexts.
This approach involves fine-tuning LLMs on task-specific instructions or prompts, guiding the model to generate outputs that align with the given instructions. By conditioning the model on explicit instructions, instruction fine-tuning facilitates more accurate and tailored responses, making LLMs more versatile and effective in various applications such as language translation, text summarization, and question answering.&lt;/p></description></item><item><title>Chapter 08.02: Chain-of-thought Prompting</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_02_cot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_02_cot/</guid><description>&lt;p>Chain of thought (CoT) prompting [1] is a prompting method that encourage Large Language Models (LLMs) to explain their reasoning. This method contrasts with standard prompting by not only seeking an answer but also requiring the model to explain its steps to arrive at that answer. By guiding the model through a logical chain of thought, chain of thought prompting encourages the generation of more structured and cohesive text, enabling LLMs to produce more accurate and informative outputs across various tasks and domains.&lt;/p></description></item><item><title>Chapter 08.03: Emergent Abilities</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_03_emerging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_03_emerging/</guid><description>&lt;p>Various researchers have reported that LLMs seem to have emergent abilities. These are sudden appearances of new abilities when Large Language Models (LLMs) are scaled up. In this section we introduce the concept of emergent abilities and discuss a potential counter argument for the concept of emergence.&lt;/p></description></item><item><title/><link>https://slds-lmu.github.io/dl4nlp/exercises/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/exercises/</guid><description>Exercises Exercise Chapter 1 Exercise Chapter 2 Exercise Chapter 3 Exercise Chapter 4 Exercise Chapter 5 Exercise Chapter 6 Exercise Chapter 7 Exercise Chapter 8 Exercise Chapter 9 Exercise Chapter 10</description></item><item><title/><link>https://slds-lmu.github.io/dl4nlp/references/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/references/</guid><description>References Your markdown comes here!</description></item><item><title>Cheat Sheets</title><link>https://slds-lmu.github.io/dl4nlp/appendix/01_cheat_sheets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/01_cheat_sheets/</guid><description>possible coming in the future ..</description></item><item><title>Errata</title><link>https://slds-lmu.github.io/dl4nlp/appendix/02_errata/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/02_errata/</guid><description>Errata in the slides shown in the videos to be added once videos + updated slides thereafter are available ðŸ˜‰</description></item><item><title>Related Courses</title><link>https://slds-lmu.github.io/dl4nlp/appendix/03_related/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/03_related/</guid><description>Other ML courses Introduction to Machine Learning (I2ML) Introduction to Deep Learning (I2DL)</description></item></channel></rss>