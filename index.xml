<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning for NLP (DL4NLP) on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/</link><description>Recent content in Deep Learning for NLP (DL4NLP) on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 0.1: ML Basics (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-01-ml-basics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-01-ml-basics/</guid><description>&lt;p>This chapter introduces the basic concepts of Machine Learning. We focus on supervised learning, explain the difference between regression and classification, show how to evaluate and compare Machine Learning models and formalize the concept of learning.&lt;/p></description></item><item><title>Chapter 0.2: Supervised Regression (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-02-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-02-regression/</guid><description>&lt;p>This chapter treats the supervised regression task in more detail. We will see different loss functions for regression, how a linear regression model can be used from a Machine Learning perspective, and how to extend it with polynomials for greater flexibility.&lt;/p></description></item><item><title>Chapter 0.3: Supervised Classification (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-03-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-03-classification/</guid><description>&lt;p>This chapter treats the supervised classification task in more detail. We will see examples of binary and multiclass classification and the differences between discriminative and generative approaches. In particular, we will address logistic regression, discriminant analysis and naive Bayes classifiers.&lt;/p></description></item><item><title>Chapter 0.4: Multiclass Classification (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-04-multiclass/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-04-multiclass/</guid><description>&lt;p>This chapter treats the multiclass case of classification. Tasks with more than two classes preclude the application of some techniques studied in the binary scenario and require an adaptation of loss functions.&lt;/p></description></item><item><title>Chapter 0.5: Evaluation (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-05-evaluation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-05-evaluation/</guid><description>&lt;p>This chapter treats the challenge of evaluating the performance of a model. We will introduce different performance measures for regression and classification tasks, explain the problem of overfitting as well as the difference between training and test error, and, lastly, present a variety of resampling techniques.&lt;/p></description></item><item><title>Chapter 01.01: Introduction and Course Outline</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_01_course_intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_01_course_intro/</guid><description>&lt;p>This chapter introduces the people responsible for the course, aims to answer all open question and should give an impression of the expected workload.&lt;/p></description></item><item><title>Chapter 01.02: Learning Paradigms</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_02_learningparadigms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_02_learningparadigms/</guid><description>&lt;p>This chapter introduces different learning paradigms, such as embeddings, prompting and pre-training and finetuning a model. These are all very important concepts in the context of Deep Learning for NLP. The purpose of this chapter is to give an overview over these concepts.&lt;/p></description></item><item><title>Chapter 01.03: NLP tasks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_03_tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_03_tasks/</guid><description>&lt;p>Here you will learn about the distinctions between low-level and high-level tasks, as well as the differences between linguistic tasks and broader classification tasks.&lt;/p></description></item><item><title>Chapter 01.04: Neural Probabalistic Language Model</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_04_nplm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_04_nplm/</guid><description>&lt;p>In this chapter, take your first steps into the world of Deep Neural Networks for language modeling with the Neural Probabilistic Language Model (NPLM) [1]. Explore how NPLM represents a foundational approach in utilizing deep learning techniques for understanding and generating natural language. Uncover the principles behind NPLM and its significance in paving the way for more sophisticated language models.&lt;/p></description></item><item><title>Chapter 01.05: Word Embeddings</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_05_embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_05_embeddings/</guid><description>&lt;p>Embeddings allow us to represent tokens with a vector representation, enabling computers to efficiently understand the meaning and context of different words. In this chapter we will first explain the general concept of embeddings and then introduce two popular approaches, namely Word2Vec [1] and FastText [2]. Word2vec is a neural network model that learns distributed representations of words in a continuous vector space based on their contextual usage in a corpus of text. Word2vec achieves this by training on a large dataset to predict the surrounding words given a target word, embedding each word as a dense vector where similar words are closer together in the vector space. FastText is an extension of Word2vec that represents words as bags of character n-grams, enabling it to capture morphological information alongside semantics. It accomplishes this by breaking down each word into character n-grams, creating embeddings for these subword units, and then summing or averaging these embeddings to obtain the representation for the whole word.&lt;/p></description></item><item><title>Chapter 02.01: Recurrent Neural Networks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_01_rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_01_rnn/</guid><description>&lt;p>Conventional Feed Forward Neural Networks donâ€™t allow us to process sequential data, which is why we need Recurrent Neural Networks (RNNs) to handle text data. In this chapter we also get to know models that help us to overcome the limits of simple RNNs. You will learn about LSTMs and Bidirectional RNNs. LSTMs incorporate different gates that control the information flow of the network and allow us to model long term dependencies. Bidirectional RNNs introduce bidirectionality into the model, which allows it to not only learn from the left side but also from the right side context of sequential data&lt;/p></description></item><item><title>Chapter 02.02 Attention</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_02_attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_02_attention/</guid><description>&lt;p>This chapter will give you a first introduction into the concept of Attention, as introduced in [1]. Attention mechanisms allow neural networks to focus on specific parts of the input sequence, assigning varying degrees of importance to different elements, enhancing performance especially in tasks where long-range dependencies are crucial, overcoming limitations of LSTMs and vanilla bidirectional RNNs which struggle with retaining information across long sequences or capturing complex relationships between distant elements. This is achieved by dynamically weighting the importance of different parts of the input sequence during computation, enabling the model to attend to relevant information and effectively process inputs of varying lengths.&lt;/p></description></item><item><title>Chapter 02.03: ELMo</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_03_elmo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_03_elmo/</guid><description>&lt;p>Here you will learn about ELMo (Embeddings from Language Models) [1], which is a deep contextualized word representation model that generates word embeddings by considering the entire input sentence, capturing complex linguistic features and contextual nuances. It accomplishes this by using a bidirectional LSTM (Long Short-Term Memory) network to generate contextualized word representations, where each word&amp;rsquo;s embedding is dynamically influenced by its surrounding context. This enables ELMo embeddings to capture polysemy, syntactic variations, and semantic nuances that traditional word embeddings like Word2vec or FastText may miss.&lt;/p></description></item><item><title>Chapter 02.04 Revisiting words: Tokenization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_04_tokenization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_04_tokenization/</guid><description>&lt;p>This chapter is about Tokenization, which is the process of breaking down a sequence of text into smaller, meaningful units, such as words or subwords, to facilitate natural language processing tasks. Various tokenization methods exist, including Byte Pair Encoding (BPE) or WordPiece, each with its own approach to dividing text into tokens. BPE and WordPiece are subword tokenization techniques that iteratively merge frequent character sequences to form larger units, effectively capturing both common words and rare morphological variations.&lt;/p></description></item><item><title>Chapter 3.1: A universal deep learning architecture</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_01_intro_trafo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_01_intro_trafo/</guid><description>&lt;p>This chapter briefly introduces different use cases of the Transformer.&lt;/p></description></item><item><title>Chapter 3.2: The Encoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_02_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_02_encoder/</guid><description>&lt;p>This chapter further elaborates on the Transformer by focusing on the Encoder part and introducing the concepts of self- and cross attention.&lt;/p></description></item><item><title>Chapter 3.3: The Decoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/</guid><description>&lt;p>This chapter is about the decoder part of the transformer and masked self attention.&lt;/p></description></item><item><title>Chapter 3.4: Long Sequences: Transformer-XL</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_04_trafo_xl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_04_trafo_xl/</guid><description>&lt;p>This chapter is about the Transformer-XL and how it deals with the issue of long sequences.&lt;/p></description></item><item><title>Chapter 3.5: Efficient Transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_efficient/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_efficient/</guid><description>&lt;p>This chapter discusses the efficiency problems and shortcomings of transformer-based models and briefly talks about ways to deal with these issues.&lt;/p></description></item><item><title>Chapter 4.1: ARLMs vs. MLM</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_01_arlm_mlm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_01_arlm_mlm/</guid><description>&lt;p>This chapter will introduce the concept of self supervision and explain the difference between &lt;strong>A&lt;/strong>utoregressive &lt;strong>L&lt;/strong>anguage &lt;strong>M&lt;/strong>odelling and &lt;strong>M&lt;/strong>asked &lt;strong>L&lt;/strong>anguage &lt;strong>M&lt;/strong>odelling.&lt;/p></description></item><item><title>Chapter 4.2: Measuring Performance</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_02_metrics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_02_metrics/</guid><description>&lt;p>Here we discuss various ways of measuring the performance of language models.&lt;/p></description></item><item><title>Chapter 4.3: The Architecture</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_03_corefacts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_03_corefacts/</guid><description>&lt;p>In this chapter we explain the architecture of BERT and how it uses the Encoder of the Transformer.&lt;/p></description></item><item><title>Chapter 4.4: Pre-training &amp; Fine-Tuning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_04_pretrain_finetune/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_04_pretrain_finetune/</guid><description>&lt;p>This chapter discusses the methods used for pre-training of BERT, such as masked language modelling and next sentence prediction. It also briefly mentions how BERT has been finetuned for different tasks.&lt;/p></description></item><item><title>Chapter 4.5: Transfer Learning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_05_transferlearning_selfsup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_05_transferlearning_selfsup/</guid><description>&lt;p>This chapter briefly introduces the concept of Transfer Learning .&lt;/p></description></item><item><title>Chapter 5.1: Implications for future work &amp; BERTology</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_01_bertology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_01_bertology/</guid><description>&lt;p>BERTs architecture was very impactful and changed research in this field. This chapter also gives a glimpse into BERTology.&lt;/p></description></item><item><title>Chapter 5.2: BERT-based architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_02_bert_based/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_02_bert_based/</guid><description>&lt;p>By changing the pretraining objectives you can create new architectures. Here we introduce RoBERTa and ALBERT.&lt;/p></description></item><item><title>Chapter 5.3: Model distillation</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_03_distilbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_03_distilbert/</guid><description>&lt;p>This chapter introduces the concept of model distillation as a way to create more efficient models.&lt;/p></description></item><item><title>Chapter 6.1: Post-BERT architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_01_postbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_01_postbert/</guid><description>&lt;p>This chapter will introduce two new architectures from the post-BERT era, namely ELECTRA and XLNet. By changing the pre training approach we can create new models.&lt;/p></description></item><item><title>Chapter 6.2: Tasks as text-to-text problem</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_02_text2text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_02_text2text/</guid><description>&lt;p>BERT has many shortcomings which is why we reformulate every classification task as a text-to-text problem.&lt;/p></description></item><item><title>Chapter 6.3: BERT-based architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_03_t5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_03_t5/</guid><description>&lt;p>Here we introduce T5 (&lt;strong>T&lt;/strong>ext-&lt;strong>T&lt;/strong>o-&lt;strong>T&lt;/strong>ext &lt;strong>T&lt;/strong>ransfer &lt;strong>T&lt;/strong>ransformer) a complete encoder-decoder Transformer architecture, which reformulates every task as text-to-text tasks.&lt;/p></description></item><item><title>Chapter 7.1: GPT-1 (2018)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_01_gpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_01_gpt/</guid><description>&lt;p>Here we introduce GPT-1 and how it uses the transformer decoder and task-specific input transformations to achieve its performance.&lt;/p></description></item><item><title>Chapter 7.2: GPT-2 (2019)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_02_gpt2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_02_gpt2/</guid><description>&lt;p>In this chapter we introduce GPT-2 as the next model and see how it acts as an universal task solver.&lt;/p></description></item><item><title>Chapter 7.3: GPT-3 (2020) &amp; X-shot learning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_03_gpt3xshot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_03_gpt3xshot/</guid><description>&lt;p>In this chapter we introduce GPT-3 and its X-shot learning capabilities. We also introduce the concept of in-context learning.&lt;/p></description></item><item><title>Chapter 7.4: Tasks &amp; Performance</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_04_tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_04_tasks/</guid><description>&lt;p>GPT-3 has X-shot abilities. This chapters purpose is to illustrate GPT-3&amp;rsquo;s performance in various X-shot setting and to giev an overview on relevant tasks and benchmarks&lt;/p></description></item><item><title>Chapter 7.5: Discussion: Ethics and Cost</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_05_discussion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_05_discussion/</guid><description>&lt;p>In this final chapter, we discuss ethical considerations and the cost of GPT-3.&lt;/p></description></item><item><title>Chapter 8.1: Instruction Fine-Tuning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_01_instruction_tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_01_instruction_tuning/</guid><description>&lt;p>In this chapter we introduce instruction-tuning, which is a technique that allows us to adapt the models to follow instructions.&lt;/p></description></item><item><title>Chapter 8.2: Chain-of-thought Prompting</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_02_cot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_02_cot/</guid><description>&lt;p>In this session we cover Chain-of-thoght Prompting, which is a technique to improve the performance of models without requiring additional training.&lt;/p></description></item><item><title>Chapter 8.3: Emergent Abilities</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_03_emerging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_03_emerging/</guid><description>&lt;p>Various researchers have reported that LLMs seem to have emergent abilities. In this section we discuss the concept of emergence in LLMs.&lt;/p></description></item><item><title>Chapter 9.1: RLHF</title><link>https://slds-lmu.github.io/dl4nlp/chapters/09_rlhf/rlhf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/09_rlhf/rlhf/</guid><description>&lt;p>Here we cover the basics of RLHF and its related application.&lt;/p></description></item><item><title/><link>https://slds-lmu.github.io/dl4nlp/exercises/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/exercises/</guid><description>Exercises Exercise Chapter 1 Exercise Chapter 2 Exercise Chapter 3 Exercise Chapter 4 Exercise Chapter 5 Exercise Chapter 6 Exercise Chapter 7 Exercise Chapter 8 Exercise Chapter 9 Exercise Chapter 10</description></item><item><title/><link>https://slds-lmu.github.io/dl4nlp/references/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/references/</guid><description>References Your markdown comes here!</description></item><item><title>Cheat Sheets</title><link>https://slds-lmu.github.io/dl4nlp/appendix/01_cheat_sheets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/01_cheat_sheets/</guid><description>possible coming in the future ..</description></item><item><title>Errata</title><link>https://slds-lmu.github.io/dl4nlp/appendix/02_errata/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/02_errata/</guid><description>Errata in the slides shown in the videos to be added once videos + updated slides thereafter are available ðŸ˜‰</description></item><item><title>Related Courses</title><link>https://slds-lmu.github.io/dl4nlp/appendix/03_related/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/03_related/</guid><description>Other ML courses Introduction to Machine Learning (I2ML) Introduction to Deep Learning (I2DL)</description></item></channel></rss>