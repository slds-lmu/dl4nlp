<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning for NLP (DL4NLP) on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/</link><description>Recent content in Deep Learning for NLP (DL4NLP) on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo 0.125.0</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 0.1: ML Basics (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-01-ml-basics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-01-ml-basics/</guid><description>&lt;p>This chapter introduces the basic concepts of Machine Learning. We focus on supervised learning, explain the difference between regression and classification, show how to evaluate and compare Machine Learning models and formalize the concept of learning.&lt;/p></description></item><item><title>Chapter 0.2: Supervised Regression (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-02-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-02-regression/</guid><description>&lt;p>This chapter treats the supervised regression task in more detail. We will see different loss functions for regression, how a linear regression model can be used from a Machine Learning perspective, and how to extend it with polynomials for greater flexibility.&lt;/p></description></item><item><title>Chapter 0.3: Supervised Classification (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-03-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-03-classification/</guid><description>&lt;p>This chapter treats the supervised classification task in more detail. We will see examples of binary and multiclass classification and the differences between discriminative and generative approaches. In particular, we will address logistic regression, discriminant analysis and naive Bayes classifiers.&lt;/p></description></item><item><title>Chapter 0.4: Multiclass Classification (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-04-multiclass/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-04-multiclass/</guid><description>&lt;p>This chapter treats the multiclass case of classification. Tasks with more than two classes preclude the application of some techniques studied in the binary scenario and require an adaptation of loss functions.&lt;/p></description></item><item><title>Chapter 0.5: Evaluation (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-05-evaluation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-05-evaluation/</guid><description>&lt;p>This chapter treats the challenge of evaluating the performance of a model. We will introduce different performance measures for regression and classification tasks, explain the problem of overfitting as well as the difference between training and test error, and, lastly, present a variety of resampling techniques.&lt;/p></description></item><item><title>Chapter 01.01: Introduction and Course Outline</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_01_course_intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_01_course_intro/</guid><description>&lt;p>This chapter introduces the people responsible for the course aims to answer all open question and should give an impression of the expected workload.&lt;/p></description></item><item><title>Chapter 01.02: Learning Paradigms</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_02_learningparadigms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_02_learningparadigms/</guid><description>&lt;p>This chapter introduces different learning paradigms, such as embeddings, prompting and pre-training &amp;amp; finetuning a model&lt;/p></description></item><item><title>Chapter 01.03: NLP tasks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_03_tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_03_tasks/</guid><description>&lt;p>This chapter discusses various NLP tasks of different types (low- vs. high-level) and purely Linguistic task vs. more general classification task.&lt;/p></description></item><item><title>Chapter 01.04: Neural Probabalistic Language Model</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_04_nplm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_04_nplm/</guid><description>&lt;p>This chapter introduces the Neural Probabalistic Language Model as a first approach to use Deep Neural Networks for language modelling.&lt;/p></description></item><item><title>Chapter 01.05: Word Embeddings</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_05_embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_05_embeddings/</guid><description>&lt;p>This chapter introduces Word Embeddings and discusses different methods of creating them, such as Word2Vec and CBOW.&lt;/p></description></item><item><title>Chapter 02.01: Recurrent Neural Networks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_01_rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_01_rnn/</guid><description>&lt;p>This chapter introduces Recurrent Neural Networks in the context of Language Modelling and discusses different types of RNNs, such as LSTMs and Bidirectional RNNs.&lt;/p></description></item><item><title>Chapter 02.02 Attention</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_02_attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_02_attention/</guid><description>&lt;p>This chapter provides a first introduction to the Attention mechanism as a way to model long range dependencies.&lt;/p></description></item><item><title>Chapter 02.03: ELMo</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_03_elmo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_03_elmo/</guid><description>&lt;p>In this chapter we introduce ELMo, a modelling approach, that enables us to contextualize word embeddings.&lt;/p></description></item><item><title>Chapter 02.04 Revisiting words: Tokenization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_04_tokenization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_04_tokenization/</guid><description>&lt;p>In order to feed text data into a model we have to tokenize it first. This chapter discusses various types of text tokenization.&lt;/p></description></item><item><title>Chapter 3.1: A universal deep learning architecture</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_01_intro_trafo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_01_intro_trafo/</guid><description>&lt;p>This chapter briefly introduces different use cases of the Transformer.&lt;/p></description></item><item><title>Chapter 3.2: The Encoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_02_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_02_encoder/</guid><description>&lt;p>This chapter further elaborates on the Transformer by focusing on the Encoder part and introducing the concepts of self- and cross attention.&lt;/p></description></item><item><title>Chapter 3.3: The Decoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/</guid><description>&lt;p>This chapter is about the decoder part of the transformer and masked self attention.&lt;/p></description></item><item><title>Chapter 3.4: Long Sequences: Transformer-XL</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_04_trafo_xl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_04_trafo_xl/</guid><description>&lt;p>This chapter is about the Transformer-XL and how it deals with the issue of long sequences.&lt;/p></description></item><item><title>Chapter 3.5: Efficient Transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_efficient/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_efficient/</guid><description>&lt;p>This chapter discusses the efficiency problems and shortcomings of transformer-based models and briefly talks about ways to deal with these issues.&lt;/p></description></item><item><title>Chapter 4.1: ARLMs vs. MLM</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_01_arlm_mlm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_01_arlm_mlm/</guid><description>&lt;p>This chapter will introduce the concept of self supervision and explain the difference between &lt;strong>A&lt;/strong>utoregressive &lt;strong>L&lt;/strong>anguage &lt;strong>M&lt;/strong>odelling and &lt;strong>M&lt;/strong>asked &lt;strong>L&lt;/strong>anguage &lt;strong>M&lt;/strong>odelling.&lt;/p></description></item><item><title>Chapter 4.2: Measuring Performance</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_02_metrics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_02_metrics/</guid><description>&lt;p>Here we discuss various ways of measuring the performance of language models.&lt;/p></description></item><item><title>Chapter 4.3: The Architecture</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_03_corefacts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_03_corefacts/</guid><description>&lt;p>In this chapter we explain the architecture of BERT and how it uses the Encoder of the Transformer.&lt;/p></description></item><item><title>Chapter 4.4: Pre-training &amp; Fine-Tuning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_04_pretrain_finetune/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_04_pretrain_finetune/</guid><description>&lt;p>This chapter discusses the methods used for pre-training of BERT, such as masked language modelling and next sentence prediction. It also briefly mentions how BERT has been finetuned for different tasks.&lt;/p></description></item><item><title>Chapter 4.5: Transfer Learning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_05_transferlearning_selfsup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_bert/04_05_transferlearning_selfsup/</guid><description>&lt;p>This chapter briefly introduces the concept of Transfer Learning .&lt;/p></description></item><item><title>Chapter 5.1: Implications for future work &amp; BERTology</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_01_bertology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_01_bertology/</guid><description>&lt;p>BERTs architecture was very impactful and changed research in this field. This chapter also gives a glimpse into BERTology.&lt;/p></description></item><item><title>Chapter 5.2: BERT-based architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_02_bert_based/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_02_bert_based/</guid><description>&lt;p>By changing the pretraining objectives you can create new architectures. Here we introduce RoBERTa and ALBERT.&lt;/p></description></item><item><title>Chapter 5.3: Model distillation</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_03_distilbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_bert_based/05_03_distilbert/</guid><description>&lt;p>This chapter introduces the concept of model distillation as a way to create more efficient models.&lt;/p></description></item><item><title>Chapter 6.1: Post-BERT architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_01_postbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_01_postbert/</guid><description>&lt;p>This chapter will introduce two new architectures from the post-BERT era, namely ELECTRA and XLNet. By changing the pre training approach we can create new models.&lt;/p></description></item><item><title>Chapter 6.2: Tasks as text-to-text problem</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_02_text2text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_02_text2text/</guid><description>&lt;p>BERT has many shortcomings which is why we reformulate every classification task as a text-to-text problem.&lt;/p></description></item><item><title>Chapter 6.3: BERT-based architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_03_t5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_post_bert_t5/06_03_t5/</guid><description>&lt;p>Here we introduce T5 (&lt;strong>T&lt;/strong>ext-&lt;strong>T&lt;/strong>o-&lt;strong>T&lt;/strong>ext &lt;strong>T&lt;/strong>ransfer &lt;strong>T&lt;/strong>ransformer) a complete encoder-decoder Transformer architecture, which reformulates every task as text-to-text tasks.&lt;/p></description></item><item><title>Chapter 7.1: GPT-1 (2018)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_01_gpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_01_gpt/</guid><description>&lt;p>Here we introduce GPT-1 and how it uses the transformer decoder and task-specific input transformations to achieve its performance.&lt;/p></description></item><item><title>Chapter 7.2: GPT-2 (2019)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_02_gpt2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_02_gpt2/</guid><description>&lt;p>In this chapter we introduce GPT-2 as the next model and see how it acts as an universal task solver.&lt;/p></description></item><item><title>Chapter 7.3: GPT-3 (2020) &amp; X-shot learning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_03_gpt3xshot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_03_gpt3xshot/</guid><description>&lt;p>In this chapter we introduce GPT-3 and its X-shot learning capabilities. We also introduce the concept of in-context learning.&lt;/p></description></item><item><title>Chapter 7.4: Tasks &amp; Performance</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_04_tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_04_tasks/</guid><description>&lt;p>GPT-3 has X-shot abilities. This chapters purpose is to illustrate GPT-3&amp;rsquo;s performance in various X-shot setting and to giev an overview on relevant tasks and benchmarks&lt;/p></description></item><item><title>Chapter 7.5: Discussion: Ethics and Cost</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_05_discussion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_gpt/07_05_discussion/</guid><description>&lt;p>In this final chapter, we discuss ethical considerations and the cost of GPT-3.&lt;/p></description></item><item><title>Chapter 8.1: Instruction Fine-Tuning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_01_instruction_tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_01_instruction_tuning/</guid><description>&lt;p>In this chapter we introduce instruction-tuning, which is a technique that allows us to adapt the models to follow instructions.&lt;/p></description></item><item><title>Chapter 8.2: Chain-of-thought Prompting</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_02_cot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_02_cot/</guid><description>&lt;p>In this session we cover Chain-of-thoght Prompting, which is a technique to improve the performance of models without requiring additional training.&lt;/p></description></item><item><title>Chapter 8.3: Emergent Abilities</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_03_emerging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_llm/08_03_emerging/</guid><description>&lt;p>Various researchers have reported that LLMs seem to have emergent abilities. In this section we discuss the concept of emergence in LLMs.&lt;/p></description></item><item><title>Chapter 9.1: RLHF</title><link>https://slds-lmu.github.io/dl4nlp/chapters/09_rlhf/rlhf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/09_rlhf/rlhf/</guid><description>&lt;p>Here we cover the basics of RLHF and its related application.&lt;/p></description></item><item><title/><link>https://slds-lmu.github.io/dl4nlp/exercises/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/exercises/</guid><description>Exercises Exercise Chapter 1 Exercise Chapter 2 Exercise Chapter 3 Exercise Chapter 4 Exercise Chapter 5 Exercise Chapter 6 Exercise Chapter 7 Exercise Chapter 8 Exercise Chapter 9 Exercise Chapter 10</description></item><item><title/><link>https://slds-lmu.github.io/dl4nlp/references/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/references/</guid><description>References Your markdown comes here!</description></item><item><title>Cheat Sheets</title><link>https://slds-lmu.github.io/dl4nlp/appendix/01_cheat_sheets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/01_cheat_sheets/</guid><description>possible coming in the future ..</description></item><item><title>Errata</title><link>https://slds-lmu.github.io/dl4nlp/appendix/02_errata/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/02_errata/</guid><description>Errata in the slides shown in the videos to be added once videos + updated slides thereafter are available ðŸ˜‰</description></item><item><title>Related Courses</title><link>https://slds-lmu.github.io/dl4nlp/appendix/03_related/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/03_related/</guid><description>Other ML courses Introduction to Machine Learning (I2ML) Introduction to Deep Learning (I2DL)</description></item></channel></rss>