<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning for NLP (DL4NLP) on Deep Learning for Natural Language Processing (DL4NLP)</title><link>https://slds-lmu.github.io/dl4nlp/</link><description>Recent content in Deep Learning for NLP (DL4NLP) on Deep Learning for Natural Language Processing (DL4NLP)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/dl4nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 0.1: ML Basics (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-01-ml-basics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-01-ml-basics/</guid><description>&lt;p>This chapter introduces the basic concepts of Machine Learning. We focus on supervised learning, explain the difference between regression and classification, show how to evaluate and compare Machine Learning models and formalize the concept of learning.&lt;/p></description></item><item><title>Chapter 0.2: Supervised Regression (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-02-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-02-regression/</guid><description>&lt;p>This chapter treats the supervised regression task in more detail. We will see different loss functions for regression, how a linear regression model can be used from a Machine Learning perspective, and how to extend it with polynomials for greater flexibility.&lt;/p></description></item><item><title>Chapter 0.3: Supervised Classification (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-03-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-03-classification/</guid><description>&lt;p>This chapter treats the supervised classification task in more detail. We will see examples of binary and multiclass classification and the differences between discriminative and generative approaches. In particular, we will address logistic regression, discriminant analysis and naive Bayes classifiers.&lt;/p></description></item><item><title>Chapter 0.4: Multiclass Classification (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-04-multiclass/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-04-multiclass/</guid><description>&lt;p>This chapter treats the multiclass case of classification. Tasks with more than two classes preclude the application of some techniques studied in the binary scenario and require an adaptation of loss functions.&lt;/p></description></item><item><title>Chapter 0.5: Evaluation (I2ML)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-05-evaluation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/00_basics/00-05-evaluation/</guid><description>&lt;p>This chapter treats the challenge of evaluating the performance of a model. We will introduce different performance measures for regression and classification tasks, explain the problem of overfitting as well as the difference between training and test error, and, lastly, present a variety of resampling techniques.&lt;/p></description></item><item><title>Chapter 01.01: Introduction and Course Outline</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_01_course_intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_01_course_intro/</guid><description>&lt;p>This chapter introduces the people responsible for the course aims to answer all open question and should give an impression of the expected workload.&lt;/p></description></item><item><title>Chapter 01.02: Learning Paradigms</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_02_learningparadigms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_02_learningparadigms/</guid><description>&lt;p>This chapter introduces different learning paradigms, such as embeddings, prompting and pre-training &amp;amp; finetuning a model&lt;/p></description></item><item><title>Chapter 01.03: NLP tasks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_03_tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_03_tasks/</guid><description>&lt;p>This chapter discusses various NLP tasks of different types (low- vs. high-level) and purely Linguistic task vs. more general classification task.&lt;/p></description></item><item><title>Chapter 01.04: Neural Probabalistic Language Model</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_04_nplm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_04_nplm/</guid><description>&lt;p>This chapter introduces the Neural Probabalistic Language Model as a first approach to use Deep Neural Networks for language modelling.&lt;/p></description></item><item><title>Chapter 01.05: Word Embeddings</title><link>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_05_embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/01_introduction/01_05_embeddings/</guid><description>&lt;p>This chapter introduces Word Embeddings and discusses different methods of creating them, such as Word2Vec and CBOW.&lt;/p></description></item><item><title>Chapter 02.01: Recurrent Neural Networks</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_01_rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_01_rnn/</guid><description>&lt;p>This chapter introduces Recurrent Neural Networks in the context of Language Modelling and discusses different types of RNNs, such as LSTMs and Bidirectional RNNs.&lt;/p></description></item><item><title>Chapter 02.02 Attention</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_02_attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_02_attention/</guid><description>&lt;p>This chapter provides a first introduction to the Attention mechanism as a way to model long range dependencies.&lt;/p></description></item><item><title>Chapter 02.03: ELMo</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_03_elmo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_03_elmo/</guid><description>&lt;p>In this chapter we introduce ELMo, a modelling approach, that enables us to contextualize word embeddings.&lt;/p></description></item><item><title>Chapter 02.04 Revisiting words: Tokenization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_04_tokenization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/02_dl_basics/02_04_tokenization/</guid><description>&lt;p>In order to feed text data into a model we have to tokenize it first. This chapter discusses various types of text tokenization.&lt;/p></description></item><item><title>Chapter 3.1: A universal deep learning architecture</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_01_intro_trafo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_01_intro_trafo/</guid><description>&lt;p>This chapter briefly introduces different use cases of the Transformer.&lt;/p></description></item><item><title>Chapter 3.2: The Encoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_02_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_02_encoder/</guid><description>&lt;p>This chapter further elaborates on the Transformer by focusing on the Encoder part and introducing the concepts of self- and cross attention.&lt;/p></description></item><item><title>Chapter 3.3: The Decoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_03_decoder/</guid><description>&lt;p>This chapter is about the decoder part of the transformer and masked self attention.&lt;/p></description></item><item><title>Chapter 3.4: Long Sequences: Transformer-XL</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_04_trafo_xl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_04_trafo_xl/</guid><description>&lt;p>This chapter is about the Transformer-XL and how it deals with the issue of long sequences.&lt;/p></description></item><item><title>Chapter 3.5: Efficient Transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_efficient/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/03_transformer/03_05_efficient/</guid><description>&lt;p>This chapter discusses the efficiency problems and shortcomings of transformer-based models and briefly talks about ways to deal with these issues.&lt;/p></description></item><item><title>Chapter 4.1: Definitions and Challenges</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_defs_challenges/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_01_defs_challenges/</guid><description>&lt;p>This chapter will explain the different paradigms which are commonly subsumed under the term Transfer Learning. It is important to be able to distinguish them from one another, since each one of them comes with its own challenges as well as its own use cases, where it is specifically useful.&lt;/p></description></item><item><title>Chapter 4.2: Self-supervised pre-training</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_02_selfsup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_02_selfsup/</guid><description>&lt;p>Here we will introduce the concept of Self-Supervision, which is crucial for the current practice of Transfer Learning in NLP.
It refers to a learning paradigm located somewhere between supervised and unsupervised learning, since it exhibits characterstics of both of them.&lt;/p></description></item><item><title>Chapter 4.3: Data-driven tokenization</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_03_datadriven/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_03_datadriven/</guid><description>&lt;p>Two of the three currently most frequently used data-driven tokenization algorithms will be introduced in this chapter. The remaining one, BPE, will be covered at the beginning of the next chapter.&lt;/p></description></item><item><title>Chapter 4.4: Early Transfer Learning architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_04_00_early_transfer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_04_00_early_transfer/</guid><description>&lt;p>Before the use of the Transformer architecture (&lt;a href="https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/">cf. Chapter 5&lt;/a>) for Transfer Learning, two milestone architectures were published.&lt;/p></description></item><item><title>Chapter 4.4.1: ELMo</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_04_01_elmo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_04_01_elmo/</guid><description>&lt;p>In their paper &lt;em>Deep contextualized word representations&lt;/em>, Peters et al. introduced a new class of word embeddings called ELMo (&lt;em>E&lt;/em>mbeddings from &lt;em>L&lt;/em>anguage &lt;em>Mo&lt;/em>dels).
Their unique feature is the ability to contextualize the embeddings in a bidirectional fashion (using biLSTMs), i.e. the representation of a word depends on the context on both sides. In order to obtain embeddings, whole sentences have to be fed to this architecture and the retrieved embeddings can then be used for a specific downstream task.&lt;/p></description></item><item><title>Chapter 4.4.2: ULMFiT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_04_02_ulmfit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/04_transfer_learning/04_04_02_ulmfit/</guid><description>&lt;p>In contrast to ELMo, this architecture is not able to produce bidirectionally contextual embeddings but only unidirectionally contextual ones (since unidirectional LSTM are employed). Apart from this, is relies on a different transfer learning paradigm. Instead of &amp;ldquo;only&amp;rdquo; generating embedding which can be used for downstream tasks, the whole architecture is &lt;em>fine-tuned&lt;/em> towards a specific task. Hence the name &lt;em>U&lt;/em>niversal &lt;em>L&lt;/em>anguage &lt;em>M&lt;/em>odel &lt;em>Fi&lt;/em>ne-&lt;em>T&lt;/em>uning (ULMFiT).&lt;/p></description></item><item><title>Chapter 5.1: Byte-Pair Encoding (BPE)</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_01_bpe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_01_bpe/</guid><description>&lt;p>Understanding BPE is crucial for understanding the transformer architecture, since it is used to learn the vocabulary for the embedding layer.&lt;br>
Instead of using a simple heuristic (characters or words), the tokens which constitute the vocabulary are formed base on the training corpus.&lt;/p></description></item><item><title>Chapter 5.2: Transformer Encoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_02_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_02_encoder/</guid><description>&lt;p>Since the initial use case was machine translation (cf. &lt;a href="../05_04_use_case">Chapter 5.4&lt;/a>), the transformer is an encoder-decoder architecture.
This chapter will explain the inner workings of the encoder part.&lt;/p></description></item><item><title>Chapter 5.3: Transformer Decoder</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_03_decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_03_decoder/</guid><description>&lt;p>This chapter will explain the inner workings of the decoder part of the transformer, especially the subtleties of Masked Self Attention and the Cross-Attention between encoder and decoder.&lt;/p></description></item><item><title>Chapter 5.4: Initial Use of the Transformer</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_04_use_case/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_04_use_case/</guid><description>&lt;p>In order to provide more context to get a better understanding of these model developments, we will quickly sketch the initial use of the Transformer for Machine Translation.&lt;/p></description></item><item><title>Chapter 5.5: Transformer-XL</title><link>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_05_trafo_xl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/05_transformer/05_05_trafo_xl/</guid><description>&lt;p>The Transfomer-XL is motivated by the problem of modeling long sequences with the Transformer architecture. While the computational complexity of the Vanilla Transformer
scales quadratically with the sequence length (and is this problematic), the authors here propose a segment recurrence mechanism to handle this problem efficiently.
Further the introduce &lt;em>relative&lt;/em> positional encodings (as opposed to absolute ones in the Vanilla Tranformer) to make the segment recurrence work in a meaningful way.&lt;/p></description></item><item><title>Chapter 6.1: Transformer Decoder - GPT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_01_gpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_01_gpt/</guid><description>&lt;p>This chapter will explain how the decoder part of the transformer was first used as the backbone for pre-training a language model. Radford et al. (2018) pre-trained a multi-layer transformer decoder using the language modeling objective on a large unannotated corpus (BooksCorpus; Zhu et al., 2015).&lt;/p></description></item><item><title>Chapter 6.2: Transformer Encoder - BERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_00_bert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_00_bert/</guid><description>&lt;p>This chapter will explain how the encoder part of the transformer was first used as the backbone for pre-training a language model. Devlin et al. (2019) came
up with a new objective, since the unconstrained self-attention in the encoder makes it impossible to train an encoder-based model on the language modeling objective
in a meaningful way. The resulting model (BERT) was first published as a pre-print in 10/2018 and presented at the ACL conference in 06/2019.&lt;/p></description></item><item><title>Chapter 6.2.1: BERT - Architectural details</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_01_architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_01_architecture/</guid><description>&lt;p>This chapter will explain the architectural details of BERT - from the (context-free) embedding layer until the final classification layer.&lt;/p></description></item><item><title>Chapter 6.2.2: BERT - Pre-training regime</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_02_pretrain/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_02_pretrain/</guid><description>&lt;p>This chapter will explain how BERT was pre-trained.&lt;/p></description></item><item><title>Chapter 6.2.3: BERT - Fine-tuning procedure</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_03_finetune/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_03_finetune/</guid><description>&lt;p>This chapter will explain how BERT can be fine-tuned on a specific task using labeled training data.&lt;/p></description></item><item><title>Chapter 6.2.4: BERT - Shortcomings &amp; Critique</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_04_short/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_04_short/</guid><description>&lt;p>This chapter will explain the shortcomings of BERT and explain the critique which this approach faces.&lt;/p></description></item><item><title>Chapter 6.2.5: BERT - Implications for future work</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_05_implications/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_02_05_implications/</guid><description>&lt;p>This chapter will address the learnings and the implications for future research which researchers took away from analyzing BERT.&lt;/p></description></item><item><title>Chapter 6.3: BERT-based architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_00_bert_based/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_00_bert_based/</guid><description>&lt;p>Following the advent of BERT, a great focus in this research are was on extending, explaining or improving it.
This chapter will explain three of the most important architectures from this streamline of reseach called BERTology&lt;/p></description></item><item><title>Chapter 6.3.1: RoBERTa</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_01_roberta/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_01_roberta/</guid><description>&lt;p>This chapter is about RoBERTa, an optimized version of the initial BERT architecture.&lt;/p></description></item><item><title>Chapter 6.3.2: ALBERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_02_albert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_02_albert/</guid><description>&lt;p>This chapter is about ALBERT, a more efficient version of the initial BERT architecture du to the application of parameter sharing techniques.&lt;/p></description></item><item><title>Chapter 6.3.3: Model distillation - DistilBERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_03_distilbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_03_distilbert/</guid><description>&lt;p>This chapter is about DistilBERT, an architecture derived from the original BERT model via model distillation.
We will explain the process of model distillation itself as well as the created architecture.&lt;/p></description></item><item><title>Chapter 6.3.4: Add-On: Model Recycling</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_04_recycling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_03_04_recycling/</guid><description>&lt;p>This chapter is about about a very recent project on the question which model to choose for fine-tuning.&lt;/p></description></item><item><title>Chapter 6.4: Post-BERT architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_00_postbert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_00_postbert/</guid><description>&lt;p>This chapter covers a selection of model published after BERT, but which also still rely on the pre-train/fine-tune paradigm.&lt;/p></description></item><item><title>Chapter 6.4.1: Encoder architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_01_encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_01_encoder/</guid><description>&lt;p>This chapter will cover a set of transformer-encoder based archtiectures like ELECTRA or BigBird. At the moment, there are only slides on ELECTRA, whereas BigBird is still part of the efficient transformers slide deck and will be added here in the future.&lt;/p></description></item><item><title>Chapter 6.4.2: Decoder architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_02_decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_02_decoder/</guid><description>&lt;p>This chapter will cover a set of transformer-decoder based archtiectures, first and foremost GPT&amp;rsquo;s two successors and the XLNet architecture and its premutation language modeling objective.&lt;/p></description></item><item><title>Chapter 6.4.3: Encoder-Decoder architectures</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_03_encdec/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_04_03_encdec/</guid><description>&lt;p>This chapter will cover a set of models using the whole transformer architecture again.
(&lt;em>Note: T0 still to be added in future&lt;/em>)&lt;/p></description></item><item><title>Chapter 6.5: Efficient Transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_05_efficient/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/06_using_transformer/06_05_efficient/</guid><description>&lt;p>This chapter will cover a set of models trying to decrease the computational complexity of the self-attention mechanism in the transformer architecture,
since it represents the backbone of nearly all state-of-the-art models and thus also limits these models to some extent.&lt;/p></description></item><item><title>Chapter 7.1: Problems with BERT</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_01_bertproblems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_01_bertproblems/</guid><description>&lt;p>We investigate some problems of encoder-based transformer models, such as BERT, to understand what GPT-3 attempts to solve.&lt;/p></description></item><item><title>Chapter 7.2: Intro to GPT and X-shot learning</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_02_gptxshot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_02_gptxshot/</guid><description>&lt;p>In this chapter we introduce GPT and see how GPT makes predictions.&lt;/p></description></item><item><title>Chapter 7.3: GPT Performance</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_03_tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_03_tasks/</guid><description>&lt;p>Given GPT&amp;rsquo;s zero- and few-shot abilities, it can be used to tackle many tasks. We present an investigation of its perfomance on various types of tasks.&lt;/p></description></item><item><title>Chapter 7.4.2: Hype</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_042_limitations_historyhype/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_042_limitations_historyhype/</guid><description>&lt;p>GPT-3 created a huge hype around it. In this chapter we look at a different angles of this hype.&lt;/p></description></item><item><title>Chapter 7.4.3: Marcus &amp; David</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_043_limitations_marcusdavid/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_043_limitations_marcusdavid/</guid><description>&lt;p>Marcus &amp;amp; Davis look at multiple types of reasoning and analyze how GPT-3 deals with it.&lt;/p></description></item><item><title>Chapter 7.4.1: GPT limitations</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_041_limitations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_041_limitations/</guid><description>&lt;p>This chapter investigates various issues GPT has. This includes specific tasks and a comparison to human learning.&lt;/p></description></item><item><title>Chapter 7.5: Discussion</title><link>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_05_discussion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/07_few_zero_shot/07_05_discussion/</guid><description>&lt;p>In this final chapter, we discuss ethical considerations and the cost of GPT-3.&lt;/p></description></item><item><title>Chapter 8.1: Why Multilinguality?</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_01_why_multilinguality/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_01_why_multilinguality/</guid><description>&lt;p>So far, we have seen interesting use cases for NLP. Clearly, these can be extended to multiple languages. But there are also interesting problems across languages. In this part we present a general motivation and a classification of problem types.&lt;/p></description></item><item><title>Chapter 8.2: Cross-lingual Word Embeddings</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_02_cross_lingual_word_embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_02_cross_lingual_word_embeddings/</guid><description>&lt;p>In this session, multilingual word embeddings are discussed. We describe the two main training strategies and look at examples of models to train those. Additionally, we look at unsupervised learning of multi-lingual word embeddings.&lt;/p></description></item><item><title/><link>https://slds-lmu.github.io/dl4nlp/exercises/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/exercises/</guid><description>Exercises Exercise Chapter 1 Exercise Chapter 2 Exercise Chapter 3 Exercise Chapter 4 Exercise Chapter 5 Exercise Chapter 6 Exercise Chapter 7 Exercise Chapter 8 Exercise Chapter 9 Exercise Chapter 10</description></item><item><title/><link>https://slds-lmu.github.io/dl4nlp/references/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/references/</guid><description>References Your markdown comes here!</description></item><item><title>Chapter 8.3: (Massively) Multilingual transformers</title><link>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_03_multi_lingual_transformers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/chapters/08_multilingual/08_03_multi_lingual_transformers/</guid><description>&lt;p>As we have previously seen, transformers are the working horse for modern NLP. In this section we learn how transformers are adapted to work in multilingual settings and discuss typical issues when training those. Furthermore we look at zero-shot cross lingual transfer capabilities.&lt;/p></description></item><item><title>Cheat Sheets</title><link>https://slds-lmu.github.io/dl4nlp/appendix/01_cheat_sheets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/01_cheat_sheets/</guid><description>possible coming in the future ..</description></item><item><title>Errata</title><link>https://slds-lmu.github.io/dl4nlp/appendix/02_errata/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/02_errata/</guid><description>Errata in the slides shown in the videos to be added once videos + updated slides thereafter are available 😉</description></item><item><title>Related Courses</title><link>https://slds-lmu.github.io/dl4nlp/appendix/03_related/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/dl4nlp/appendix/03_related/</guid><description>Other ML courses Introduction to Machine Learning (I2ML) Introduction to Deep Learning (I2DL)</description></item></channel></rss>