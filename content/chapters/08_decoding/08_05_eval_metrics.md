---
title: "Chapter 08.05: Evaluation Metrics"
weight: 8005
---
Here we answer the question on how to evaluate the generated outputs in open ended text generation. We first explain **BLEU** [1] and **ROUGE** [2], which are metrics for tasks with a gold reference. Then we introduce **diversity**, **coherence** [3] and **MAUVE** [4], which are metrics for tasks without a gold reference such as open ended text generation. You will also learn about human evaluation.  


<!--more-->

### Lecture Slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter12-decoding/slides-125-eval_metrics.pdf" >}}

### References

- [1] [Papineni et al., 2002](https://aclanthology.org/P02-1040.pdf)
- [2] [Lin, 2004](https://aclanthology.org/W04-1013/)
- [3] [Su et al., 2022](https://arxiv.org/abs/2202.06417) 
- [4] [Pillutla et al., 2021](https://arxiv.org/abs/2102.01454)