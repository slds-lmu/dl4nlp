---
title: "Chapter 11.02: How to reduce memory and compute?" 
weight: 1102
---

Here you will learn about ways to reduce the memory and compute requirements for big models. We introduce distributed training, where you make use of data- and tensor parallellism, and FlashAttention, a method to perform attention more efficiently.

<!--more-->

### Lecture Slides 

{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter11-training-llms/slides-112-reduce-comp.pdf" >}}