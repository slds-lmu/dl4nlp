---
title: "Chapter 9: Reinforcement Learning from Human Feedback (RLHF)"
---

In the context of natural language processing (NLP), RLHF (Reinforcement Learning from Human Feedback) involves training language models to generate text or perform tasks based on evaluative signals provided by human annotators or users. This technique allows NLP models to learn from human feedback, such as ratings or corrections, to improve their language understanding, generation, or task performance. By iteratively adjusting model parameters to maximize the reward signal derived from human feedback, RLHF enables models to adapt to specific preferences or requirements, leading to more accurate and contextually relevant outputs in various NLP applications.  

### Lecture Slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter09-rlhf/slides-91-rlhf.pdf" >}}

### Additional Resources

- [Video Explaining RLHF](https://www.youtube.com/watch?v=qGyFrqc34yc)