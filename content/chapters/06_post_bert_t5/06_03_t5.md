---
title: "Chapter 06.03: Text-to-Text Transfer Transformer"
weight: 6003
---
T5 (Text-To-Text Transfer Transformer) [1] aims to unify various natural language processing tasks by framing them all as text-to-text transformations, simplifying model architectures and enabling flexible training across diverse tasks.
It achieves this by formulating input-output pairs for different tasks as text sequences, allowing the model to learn to generate target text from source text regardless of the specific task, facilitating multitask learning and transfer learning across tasks with a single, unified architecture.


<!--more-->

<!--
### Lecture video
{{< video id="TfrSKiOecWI" >}}
-->

### Lecture Slides
{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter06-post-bert-t5/slides-63-t5.pdf" >}}

### References

- [1] [Raffel et al., 2019](https://arxiv.org/abs/1910.10683)

### Additional Resources 

- [Blogpost about T5](https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51)



