---
title: "Chapter 6.3: BERT-based architectures"
weight: 6003
---
Here we introduce T5 (**T**ext-**T**o-**T**ext **T**ransfer **T**ransformer) a complete encoder-decoder Transformer architecture, which reformulates every task as text-to-text tasks.

<!--more-->

<!--
### Lecture video
{{< video id="TfrSKiOecWI" >}}
-->

### Lecture Slides
{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter06-post-bert-t5/slides-63-t5.pdf" >}}

