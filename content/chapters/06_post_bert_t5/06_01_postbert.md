---
title: "Chapter 06.01: Post-BERT architectures"
weight: 6001
---
This chapter will introduce two new architectures from the post-BERT era, namely ELECTRA [1] and XLNet [2]. By changing the pre training approach we can create new models. For ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately), the basic idea is to train a discriminator model to distinguish between the original tokens in a text sequence and replaced tokens generated by a generator model, aiming to create a more efficient pretraining approach compared to masked language modeling (MLM).
For XLNet, the basic idea is to overcome the limitations of unidirectional and bidirectional language models by introducing a permutation-based pre-training objective, the so called permutation language modeling (PLM), that enables the model to consider all possible permutations of the input tokens, capturing bidirectional context.

<!--more-->

<!--
### Lecture video
{{< video id="TfrSKiOecWI" >}}
-->

### Lecture Slides
{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter06-post-bert-t5/slides-61-postbert.pdf" >}}

### References 

- [1] [Clark et al., 2020](https://arxiv.org/abs/2003.10555)
- [2] [Yang et al., 2020](https://arxiv.org/abs/1906.08237)

### Additional Resources

- [Structured overview on BERT-related papers](https://github.com/tomohideshibata/BERT-related-papers)
- [Blogpost about ELECTRA](https://sh-tsang.medium.com/brief-review-electra-pre-training-text-encoders-as-discriminators-rather-than-generators-9568050d3a86)
- [Blogpost about XLNet](https://medium.com/@zxiao2015/understanding-language-using-xlnet-with-autoregressive-pre-training-9c86e5bea443)

