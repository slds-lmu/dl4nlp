---
title: "Chapter 02.03: ELMo"
weight: 2003
---
Here you will learn about ELMo (Embeddings from Language Models) [1], which is a deep contextualized word representation model that generates word embeddings by considering the entire input sentence, capturing complex linguistic features and contextual nuances. It accomplishes this by using a bidirectional LSTM (Long Short-Term Memory) network to generate contextualized word representations, where each word's embedding is dynamically influenced by its surrounding context. This enables ELMo embeddings to capture polysemy, syntactic variations, and semantic nuances that traditional word embeddings like Word2vec or FastText may miss.

<!--more-->

### Lecture slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter02-deeplearningbasics/slides-23-elmo.pdf" >}}

### References

- [1] [Peters et al., 2018](https://arxiv.org/abs/1802.05365)

### Additional Resources

- [ELMo Blogpost](https://sh-tsang.medium.com/review-elmo-deep-contextualized-word-representations-8eb1e58cd25c)

