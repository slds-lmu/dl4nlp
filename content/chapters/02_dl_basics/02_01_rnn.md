---
title: "Chapter 02.01: Recurrent Neural Networks"
weight: 2001

---
Conventional Feed Forward Neural Networks donâ€™t allow us to process sequential data, which is why we need Recurrent Neural Networks (RNNs) to handle text data. In this chapter we also get to know models that help us to overcome the limits of simple RNNs.  You will learn about LSTMs and Bidirectional RNNs. LSTMs incorporate different gates that control the information flow of the network and allow us to model long term dependencies. Bidirectional RNNs introduce bidirectionality into the model, which allows it to not only learn from the left side but also from the right side context of sequential data

<!--more-->

### Lecture slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter02-deeplearningbasics/slides-21-rnn.pdf" >}}

### Additional Resources 

- [Video explaining LSTM](https://www.youtube.com/watch?v=YCzL96nL7j0)

