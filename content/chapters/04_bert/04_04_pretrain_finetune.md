---
title: "Chapter 4.4: Pre-training & Fine-Tuning"
weight: 4040
---
This chapter discusses the methods used for pre-training of BERT, such as masked language modelling and next sentence prediction. It also briefly mentions how BERT has been finetuned for different tasks.

<!--more-->

### Lecture Slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter04-bert/slides-44-pretrain-finetune.pdf" >}}
