---
title: "Chapter 04.03: The Architecture"
weight: 4030
---
The architecture of BERT primarily revolves around transformer encoders, where stacked layers of self-attention mechanisms and feedforward neural networks are employed to generate contextualized representations of tokens in bidirectional context. During pre-training, BERT utilizes masked language modeling (MLM) and next sentence prediction (NSP) tasks to fine-tune the parameters of the transformer encoder layers, enabling the model to effectively capture semantic relationships and contextual nuances in text data. 

<!--more-->

<!--
### Lecture video

{{< video id="TfrSKiOecWI" >}}
-->

### Lecture Slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter04-bert/slides-43-corefacts.pdf" >}}

### Additional Resources 

- [Very good video explaining BERT](https://www.youtube.com/watch?v=90mGPxR2GgY)
