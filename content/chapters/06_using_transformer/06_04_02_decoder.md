---
title: "Chapter 6.4.2: Decoder architectures"
weight: 6042
---
This chapter will cover a set of transformer-decoder based archtiectures, first and foremost GPT's two successors and the XLNet architecture and its premutation language modeling objective.

<!--more-->

### Concept 
5 slides, 10 minutes

<!--
### Lecture video
{{< video id="TfrSKiOecWI" >}}
### Lecture Slides
{{< pdfjs file="https://github.com/slds-lmu/lecture_i2ml/blob/master/slides-pdf/slides-basics-whatisml.pdf" >}}
-->

### References 

- [Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Yang et al. (2019)](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)
- [Brown et al. (2020)](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
