---
title: "Chapter 6.4.2: Decoder architectures"
weight: 6042
---
This chapter will cover a set of transformer-decoder based archtiectures, first and foremost GPT's two successors and the XLNet architecture and its premutation language modeling objective.

<!--more-->

<!--
### Lecture video
{{< video id="TfrSKiOecWI" >}}
-->

### Lecture Slides
{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter6-usingtrafo/slides-643-xlnet.pdf" >}} 
{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter6-usingtrafo/slides-644-gpt2.pdf" >}}  

### References 

- [Yang et al. (2019)](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)
- [Radford et al. (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
