---
title: "Chapter 6.3.3: Model distillation - DistilBERT"
weight: 6033
---
This chapter is about DistilBERT, an architecture derived from the original BERT model via model distillation. 
We will explain the process of model distillation itself as well as the created architecture.

<!--more-->

### Lecture Slides
{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter6-usingtrafo/slides-633-distilbert.pdf" >}}

### References 

- [Bucila et al. (2006)](http://www.niculescu-mizil.org/papers/rtpp364-bucila.rev2.pdf)
- [Hinton et al. (2015)](https://arxiv.org/pdf/1503.02531.pdf)
- [Sanh et al. (2019)](https://arxiv.org/pdf/1910.01108.pdf)
