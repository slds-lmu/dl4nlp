---
title: "Chapter 6.5: Efficient Transformers"
weight: 6050
---
This chapter will cover a set of models trying to decrease the computational complexity of the self-attention mechanism in the transformer architecture,
since it represents the backbone of nearly all state-of-the-art models and thus also limits these models to some extent.

<!--more-->

<!--
### Lecture video
{{< video id="TfrSKiOecWI" >}}
-->

### Lecture Slides
{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter6-usingtrafo/slides-650-efficient.pdf" >}}  

### References 

- [Tay et al. (2020a)](https://arxiv.org/pdf/2009.06732.pdf)
- [Tay et al. (2020b)](https://arxiv.org/pdf/2011.04006.pdf)
