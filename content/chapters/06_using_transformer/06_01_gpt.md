---
title: "Chapter 6.1: Transformer Decoder - GPT"
weight: 6001
---
This chapter will explain how the decoder part of the transformer was first used as the backbone for pre-training a language model. Radford et al. (2018) pre-trained a multi-layer transformer decoder using the language modeling objective on a large unannotated corpus (BooksCorpus; Zhu et al., 2015).

<!--more-->

<!--
### Lecture video
{{< video id="TfrSKiOecWI" >}}
-->

### Lecture Slides
{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter6-usingtrafo/slides-61-gpt.pdf" >}}

### References 

- [Radford et al. (2018)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
