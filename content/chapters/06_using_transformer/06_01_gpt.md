---
title: "Chapter 6.1: Transformer Decoder - GPT"
weight: 6001
---
This chapter will explain how the decoder part of the transformer was first used as the backbone for pre-training a language model. Radford et al. (2018) pre-trained a multi-layer transformer decoder using the language modeling objective on a large unannotated corpus (BooksCorpus; Zhu et al., 2015).

<!--more-->

### Concept 
5 slides, 10 minutes

<!--
### Lecture video
{{< video id="TfrSKiOecWI" >}}
### Lecture Slides
{{< pdfjs file="https://github.com/slds-lmu/lecture_i2ml/blob/master/slides-pdf/slides-basics-whatisml.pdf" >}}
-->

### References 

- [Radford et al. (2018)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
