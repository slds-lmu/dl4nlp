---
title: "Chapter 6.4: Post-BERT architectures"
weight: 6040
---
This chapter covers a selection of model published after BERT, but which also still rely on the pre-train/fine-tune paradigm.

<!--more-->

<!--
### Lecture video
{{< video id="TfrSKiOecWI" >}}
-->

### Lecture Slides
{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter6-usingtrafo/slides-640-postbert.pdf" >}}
