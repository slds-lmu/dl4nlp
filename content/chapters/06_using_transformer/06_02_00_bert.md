---
title: "Chapter 6.2: Transformer Encoder - BERT"
weight: 6020
---
This chapter will explain how the encoder part of the transformer was first used as the backbone for pre-training a language model. Devlin et al. (2019) came
up with a new objective, since the unconstrained self-attention in the encoder makes it impossible to train an encoder-based model on the language modeling objective
in a meaningful way. The resulting model (BERT) was first published as a pre-print in 10/2018 and presented at the ACL conference in 06/2019.

<!--more-->

### References 

- [Devlin et al. (2018)](https://arxiv.org/pdf/1810.04805.pdf) - pre-print
- [Devlin et al. (2019)](https://aclanthology.org/N19-1423.pdf) - peer-reviewed
