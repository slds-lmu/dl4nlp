---
title: "Chapter 5.2: Transformer Encoder"
weight: 5002
---
Since the initial use case was machine translation (cf. [Chapter 5.4](../05_04_use_case)), the transformer is an encoder-decoder architecture.
This chapter will explain the inner workings of the encoder part.

## Concept 
10 slides, 15 minutes

<!--more
{{< video id="TfrSKiOecWI" >}}

{{< pdfjs file="https://github.com/slds-lmu/lecture_i2ml/blob/master/slides-pdf/slides-basics-whatisml.pdf" >}}
-->

## References 

- [Vaswani et al. (2017)]([https://www.derczynski.com/papers/archive/BPE_Gage.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf))
