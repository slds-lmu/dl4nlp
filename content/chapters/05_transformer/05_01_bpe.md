---
title: "Chapter 5.1: Byte-Pair Encoding (BPE)"
weight: 5001
---
Understanding BPE is crucial for understanding the transformer architecture, since it is used to 'learn' the vocabulary for the embedding layer.  
Instead of using a simple heuristic (characters or words), the tokens which constitute the vocabulary are formed base on the training corpus.

<!--more-->

### Concept 
3 slides, + 1 slides w/ example of 2-3 steps of merging, 8 minutes

<!--
### Lecture video

{{< video id="TfrSKiOecWI" >}}

### Lecture Slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_i2ml/blob/master/slides-pdf/slides-basics-whatisml.pdf" >}}
-->

### References 

- [Gage (1994)](https://www.derczynski.com/papers/archive/BPE_Gage.pdf)
- [Sennrich et al. (2016)](https://aclanthology.org/P16-1162.pdf)
