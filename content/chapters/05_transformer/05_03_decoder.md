---
title: "Chapter 5.3: Transformer Decoder"
weight: 5003
---
This chapter will explain the inner workings of the decoder part of the transformer, especially the subtleties of Masked Self Attention and the Cross-Attention between encoder and decoder.

<!--more-->

### Concept 
5 slides, 10 minutes

<!--
### Lecture video

{{< video id="TfrSKiOecWI" >}}
-->

### Lecture Slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter5-transformer/slides-53-decoder.pdf" >}}

### References 

- [Vaswani et al. (2017)](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
