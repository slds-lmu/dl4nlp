---
title: "Chapter 4.4.1: ELMo"
weight: 4041
---
In their paper _Deep contextualized word representations_, Peters et al. introduced a new class of word embeddings called ELMo (*E*mbeddings from *L*anguage *Mo*dels).
Their unique feature is the ability to contextualize the embeddings in a bidirectional fashion (using biLSTMs), i.e. the representation of a word depends on the context on both sides. In order to obtain embeddings, whole sentences have to be fed to this architecture and the retrieved embeddings can then be used for a specific downstream task.

<!--more-->

<!--
### Lecture video
{{< video id="TfrSKiOecWI" >}}
-->

### Lecture Slides
{{< pdfjs file="https://github.com/slds-lmu/lecture_dl4nlp/blob/main/slides/chapter4-transferlearning/slides-441-elmo.pdf" >}}

### References 

- [Peters et al. (2018)](https://aclanthology.org/N18-1202.pdf)
- [ELMo variant on huggingface](https://huggingface.co/allenai/bidaf-elmo)
